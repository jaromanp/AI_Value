{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation\\QLYS\n",
      "Interpolation\\QLYS\\InterpolatedNumWeekQLYS.csv\n",
      "Interpolation\\QLYS\\modelNumQLYS.json\n",
      "Interpolation\\QLYS\\modelNumQLYS.h5\n",
      "Interpolation\\QLYS\\QLYS_metricsNum.txt\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "# define the path\n",
    "directory = pathlib.Path('Interpolation')\n",
    "\n",
    "# define the pattern\n",
    "currentCompany = 'QLYS'\n",
    "\n",
    "for currentFile in directory.glob(currentCompany):\n",
    "    print(currentFile)\n",
    "\n",
    "fileNum = 'InterpolatedNumWeek' + str(currentCompany) + '.csv'\n",
    "modelNum = 'modelNum' + str(currentCompany) + '.json'\n",
    "weightsNum = 'modelNum' + str(currentCompany) + '.h5'\n",
    "metricsNum = str(currentCompany) + '_metricsNum' +  '.txt'\n",
    "\n",
    "print(currentFile/fileNum)\n",
    "print(currentFile/modelNum)\n",
    "print(currentFile/weightsNum)\n",
    "print(currentFile/metricsNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(currentFile/fileNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATES</th>\n",
       "      <th>U Revenue</th>\n",
       "      <th>D CR</th>\n",
       "      <th>U OE</th>\n",
       "      <th>D NOI</th>\n",
       "      <th>D CAPEX</th>\n",
       "      <th>D WK</th>\n",
       "      <th>U FCF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>4492.109972</td>\n",
       "      <td>812.480181</td>\n",
       "      <td>2489.418540</td>\n",
       "      <td>75.145098</td>\n",
       "      <td>577.788158</td>\n",
       "      <td>8017.452837</td>\n",
       "      <td>6771.043993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-08</td>\n",
       "      <td>4515.176248</td>\n",
       "      <td>807.000714</td>\n",
       "      <td>2502.985842</td>\n",
       "      <td>79.924560</td>\n",
       "      <td>583.136653</td>\n",
       "      <td>8500.842362</td>\n",
       "      <td>6780.525768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-15</td>\n",
       "      <td>4538.242524</td>\n",
       "      <td>801.521247</td>\n",
       "      <td>2516.553145</td>\n",
       "      <td>84.704022</td>\n",
       "      <td>588.485148</td>\n",
       "      <td>8984.231887</td>\n",
       "      <td>6790.007542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-22</td>\n",
       "      <td>4561.308799</td>\n",
       "      <td>796.041780</td>\n",
       "      <td>2530.120447</td>\n",
       "      <td>89.483484</td>\n",
       "      <td>593.833644</td>\n",
       "      <td>9467.621412</td>\n",
       "      <td>6799.489317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-29</td>\n",
       "      <td>4584.375075</td>\n",
       "      <td>790.562313</td>\n",
       "      <td>2543.687750</td>\n",
       "      <td>94.262946</td>\n",
       "      <td>599.182139</td>\n",
       "      <td>9951.010937</td>\n",
       "      <td>6808.971092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>2020-03-08</td>\n",
       "      <td>6591.906087</td>\n",
       "      <td>1230.012749</td>\n",
       "      <td>2744.276015</td>\n",
       "      <td>403.778199</td>\n",
       "      <td>955.546275</td>\n",
       "      <td>17099.247695</td>\n",
       "      <td>7312.022048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>2020-03-15</td>\n",
       "      <td>6562.709140</td>\n",
       "      <td>1238.215573</td>\n",
       "      <td>2728.498558</td>\n",
       "      <td>401.428594</td>\n",
       "      <td>957.067133</td>\n",
       "      <td>17091.208516</td>\n",
       "      <td>7321.278589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>6533.512193</td>\n",
       "      <td>1246.418398</td>\n",
       "      <td>2712.721101</td>\n",
       "      <td>399.078990</td>\n",
       "      <td>958.587992</td>\n",
       "      <td>17083.169337</td>\n",
       "      <td>7330.535130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>2020-03-29</td>\n",
       "      <td>6504.315246</td>\n",
       "      <td>1254.621222</td>\n",
       "      <td>2696.943644</td>\n",
       "      <td>396.729385</td>\n",
       "      <td>960.108850</td>\n",
       "      <td>17075.130158</td>\n",
       "      <td>7339.791671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>2020-04-05</td>\n",
       "      <td>6475.118299</td>\n",
       "      <td>1262.824046</td>\n",
       "      <td>2681.166188</td>\n",
       "      <td>394.379780</td>\n",
       "      <td>961.629709</td>\n",
       "      <td>17067.090979</td>\n",
       "      <td>7349.048211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          DATES    U Revenue         D CR         U OE       D NOI  \\\n",
       "0    2017-01-01  4492.109972   812.480181  2489.418540   75.145098   \n",
       "1    2017-01-08  4515.176248   807.000714  2502.985842   79.924560   \n",
       "2    2017-01-15  4538.242524   801.521247  2516.553145   84.704022   \n",
       "3    2017-01-22  4561.308799   796.041780  2530.120447   89.483484   \n",
       "4    2017-01-29  4584.375075   790.562313  2543.687750   94.262946   \n",
       "..          ...          ...          ...          ...         ...   \n",
       "166  2020-03-08  6591.906087  1230.012749  2744.276015  403.778199   \n",
       "167  2020-03-15  6562.709140  1238.215573  2728.498558  401.428594   \n",
       "168  2020-03-22  6533.512193  1246.418398  2712.721101  399.078990   \n",
       "169  2020-03-29  6504.315246  1254.621222  2696.943644  396.729385   \n",
       "170  2020-04-05  6475.118299  1262.824046  2681.166188  394.379780   \n",
       "\n",
       "        D CAPEX          D WK        U FCF  \n",
       "0    577.788158   8017.452837  6771.043993  \n",
       "1    583.136653   8500.842362  6780.525768  \n",
       "2    588.485148   8984.231887  6790.007542  \n",
       "3    593.833644   9467.621412  6799.489317  \n",
       "4    599.182139   9951.010937  6808.971092  \n",
       "..          ...           ...          ...  \n",
       "166  955.546275  17099.247695  7312.022048  \n",
       "167  957.067133  17091.208516  7321.278589  \n",
       "168  958.587992  17083.169337  7330.535130  \n",
       "169  960.108850  17075.130158  7339.791671  \n",
       "170  961.629709  17067.090979  7349.048211  \n",
       "\n",
       "[171 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6783.648075841814"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"U FCF\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['2017-01-01', 4492.10997193969, 812.480180954865, ...,\n",
       "        577.78815802641, 8017.452837404109, 6771.043992620339],\n",
       "       ['2017-01-08', 4515.176247774698, 807.0007139424814, ...,\n",
       "        583.1366531903212, 8500.842362388677, 6780.525767541855],\n",
       "       ['2017-01-15', 4538.242523609707, 801.5212469300978, ...,\n",
       "        588.4851483542328, 8984.231887373246, 6790.007542463372],\n",
       "       ...,\n",
       "       ['2020-03-22', 6533.512192865713, 1246.4183975295325, ...,\n",
       "        958.5879918424016, 17083.169337051182, 7330.535129819929],\n",
       "       ['2020-03-29', 6504.315245782211, 1254.6212218187661, ...,\n",
       "        960.1088502124788, 17075.130158238542, 7339.791670529345],\n",
       "       ['2020-04-05', 6475.11829869871, 1262.824046108, ...,\n",
       "        961.629708582556, 17067.090979425902, 7349.04821123876]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6771.043992620339 6780.525767541855 6790.007542463372 6799.4893173848905\n",
      " 6808.971092306408 6818.452867227924 6827.934642149441 6837.416417070957\n",
      " 6846.8981919924745 6856.379966913992 6865.861741835509 6875.343516757025\n",
      " 6884.825291678542 6894.3070666000585 6855.1978703980185 6816.088674195979\n",
      " 6776.979477993939 6737.870281791899 6698.761085589858 6659.651889387818\n",
      " 6620.54269318578 6581.43349698374 6542.3243007817 6503.215104579661\n",
      " 6464.105908377621 6424.99671217558 6385.88751597354 6352.937352072768\n",
      " 6319.987188171994 6287.037024271221 6254.086860370448 6221.1366964696745\n",
      " 6188.186532568901 6155.236368668127 6122.286204767354 6089.336040866582\n",
      " 6056.385876965809 6023.435713065035 5990.485549164263 5957.53538526349\n",
      " 6115.117760500664 6272.7001357378385 6430.282510975012 6587.864886212185\n",
      " 6745.447261449361 6903.029636686533 7060.612011923708 7218.194387160881\n",
      " 7375.776762398055 7533.35913763523 7690.941512872404 7848.5238881095775\n",
      " 8006.106263346751 7967.495620159019 7928.884976971291 7890.274333783558\n",
      " 7851.663690595828 7813.053047408097 7774.442404220366 7735.831761032636\n",
      " 7697.221117844903 7658.610474657174 7619.999831469443 7581.389188281711\n",
      " 7542.778545093981 7504.16790190625 7454.580683584756 7404.993465263263\n",
      " 7355.406246941769 7305.819028620274 7256.23181029878 7206.644591977288\n",
      " 7157.0573736557935 7107.470155334299 7057.882937012805 7008.295718691313\n",
      " 6958.7085003698185 6909.121282048324 6859.53406372683 6857.841104725817\n",
      " 6856.148145724804 6854.4551867237915 6852.762227722777 6851.069268721764\n",
      " 6849.376309720751 6847.683350719739 6845.990391718726 6844.297432717713\n",
      " 6842.604473716699 6840.911514715686 6839.218555714673 6837.52559671366\n",
      " 6865.057594558011 6892.589592402362 6920.121590246714 6947.653588091066\n",
      " 6975.185585935417 7002.7175837797695 7030.24958162412 7057.78157946847\n",
      " 7085.313577312822 7112.845575157174 7140.377573001526 7167.909570845876\n",
      " 7195.441568690228 7222.97356653458 7083.707959832741 6944.442353130899\n",
      " 6805.17674642906 6665.911139727221 6526.645533025379 6387.37992632354\n",
      " 6248.114319621701 6108.848712919859 5969.583106218021 5830.317499516181\n",
      " 5691.051892814339 5551.7862861125 5614.872036947715 5677.95778778293\n",
      " 5741.043538618144 5804.129289453358 5867.215040288573 5930.300791123786\n",
      " 5993.386541959001 6056.4722927942175 6119.558043629431 6182.643794464646\n",
      " 6245.72954529986 6308.815296135074 6371.901046970289 6364.747601742329\n",
      " 6357.59415651437 6350.440711286411 6343.287266058453 6336.133820830492\n",
      " 6328.980375602533 6321.826930374576 6314.673485146615 6307.520039918656\n",
      " 6300.366594690697 6293.2131494627365 6286.059704234778 6278.906259006819\n",
      " 6271.7528137788595 6345.365149797129 6418.977485815398 6492.589821833667\n",
      " 6566.202157851936 6639.814493870205 6713.4268298884745 6787.039165906745\n",
      " 6860.651501925014 6934.263837943283 7007.876173961551 7081.48850997982\n",
      " 7155.100845998091 7228.713182016358 7237.969722725775 7247.22626343519\n",
      " 7256.482804144604 7265.739344854021 7274.995885563438 7284.252426272851\n",
      " 7293.508966982266 7302.7655076916835 7312.022048401097 7321.278589110514\n",
      " 7330.535129819929 7339.791670529345 7349.04821123876]\n"
     ]
    }
   ],
   "source": [
    "X = dataset[:,1:7]\n",
    "Y = dataset[:,7]\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scale = min_max_scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.07632478, 0.20013126, 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00869664, 0.07045364, 0.2085143 , 0.00973564, 0.01393412,\n",
       "        0.02173475],\n",
       "       [0.01739328, 0.06458251, 0.21689734, 0.01947128, 0.02786824,\n",
       "        0.0434695 ],\n",
       "       ...,\n",
       "       [0.76966646, 0.54128071, 0.33810676, 0.65984509, 0.99207559,\n",
       "        0.40762382],\n",
       "       [0.75865839, 0.55006986, 0.32835809, 0.655059  , 0.9960378 ,\n",
       "        0.40726236],\n",
       "       [0.74765031, 0.55885902, 0.31860942, 0.65027292, 1.        ,\n",
       "        0.40690089]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.07632478, 0.20013126, 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scale[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136, 6) (17, 6) (18, 6) (136,) (17,) (18,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y, test_size=0.2)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\n",
    "print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(6, activation='elu', input_shape=(6,)),\n",
    "    Dense(256, activation='elu'),\n",
    "    Dense(256, activation='elu'),\n",
    "    Dense(256, activation='elu'),\n",
    "    Dense(256, activation='elu'),\n",
    "    Dense(256, activation='elu'),\n",
    "    Dense(256, activation='elu'),\n",
    "    Dense(1, activation='elu'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 136 samples, validate on 17 samples\n",
      "Epoch 1/1200\n",
      "136/136 [==============================] - 1s 10ms/step - loss: 3054.6052 - val_loss: 978.4017\n",
      "Epoch 2/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 1151.8959 - val_loss: 938.3090\n",
      "Epoch 3/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 885.6919 - val_loss: 824.0312\n",
      "Epoch 4/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 751.9101 - val_loss: 564.2721\n",
      "Epoch 5/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 680.1143 - val_loss: 1150.1195\n",
      "Epoch 6/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 699.9442 - val_loss: 508.7798\n",
      "Epoch 7/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 591.5754 - val_loss: 636.8454\n",
      "Epoch 8/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 620.6307 - val_loss: 595.1439\n",
      "Epoch 9/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 567.8310 - val_loss: 707.3496\n",
      "Epoch 10/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 567.1667 - val_loss: 499.4863\n",
      "Epoch 11/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 571.8147 - val_loss: 631.6329\n",
      "Epoch 12/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 560.8911 - val_loss: 502.9556\n",
      "Epoch 13/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 615.3525 - val_loss: 899.5364\n",
      "Epoch 14/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 545.4740 - val_loss: 555.9641\n",
      "Epoch 15/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 560.4296 - val_loss: 532.7247\n",
      "Epoch 16/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 576.0350 - val_loss: 1024.3820\n",
      "Epoch 17/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 512.6823 - val_loss: 730.5088\n",
      "Epoch 18/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 532.6310 - val_loss: 482.3439\n",
      "Epoch 19/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 480.9481 - val_loss: 573.0800\n",
      "Epoch 20/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 526.4131 - val_loss: 547.9890\n",
      "Epoch 21/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 516.1627 - val_loss: 412.7539\n",
      "Epoch 22/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 553.0401 - val_loss: 687.3427\n",
      "Epoch 23/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 509.2628 - val_loss: 403.6819\n",
      "Epoch 24/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 507.4953 - val_loss: 525.3449\n",
      "Epoch 25/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 488.9263 - val_loss: 616.7637\n",
      "Epoch 26/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 531.2451 - val_loss: 391.6111\n",
      "Epoch 27/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 508.0638 - val_loss: 523.5352\n",
      "Epoch 28/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 495.7133 - val_loss: 420.8786\n",
      "Epoch 29/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 482.4493 - val_loss: 389.6907\n",
      "Epoch 30/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 503.8266 - val_loss: 675.3494\n",
      "Epoch 31/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 504.8326 - val_loss: 412.5154\n",
      "Epoch 32/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 451.2551 - val_loss: 370.5907\n",
      "Epoch 33/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 484.6774 - val_loss: 417.7349\n",
      "Epoch 34/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 465.3717 - val_loss: 574.2559\n",
      "Epoch 35/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 461.6043 - val_loss: 540.7764TA: 0s - loss: 461.663\n",
      "Epoch 36/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 512.3339 - val_loss: 389.4694\n",
      "Epoch 37/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 479.8102 - val_loss: 416.2891\n",
      "Epoch 38/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 486.8988 - val_loss: 362.6169\n",
      "Epoch 39/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 485.6468 - val_loss: 417.4067\n",
      "Epoch 40/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 484.1362 - val_loss: 481.8468\n",
      "Epoch 41/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 464.7193 - val_loss: 370.4218\n",
      "Epoch 42/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 450.7745 - val_loss: 485.6031\n",
      "Epoch 43/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 430.7578 - val_loss: 442.3070\n",
      "Epoch 44/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 446.5397 - val_loss: 449.2670\n",
      "Epoch 45/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 456.8616 - val_loss: 377.1207\n",
      "Epoch 46/1200\n",
      "136/136 [==============================] - ETA: 0s - loss: 452.817 - 1s 4ms/step - loss: 463.7922 - val_loss: 411.8290\n",
      "Epoch 47/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 495.6202 - val_loss: 421.7152\n",
      "Epoch 48/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 444.6635 - val_loss: 614.7328\n",
      "Epoch 49/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 500.3827 - val_loss: 770.6195\n",
      "Epoch 50/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 492.7800 - val_loss: 418.8994\n",
      "Epoch 51/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 479.2358 - val_loss: 375.4783\n",
      "Epoch 52/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 433.8525 - val_loss: 601.1811\n",
      "Epoch 53/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 442.8908 - val_loss: 462.8817\n",
      "Epoch 54/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 431.4464 - val_loss: 592.6440\n",
      "Epoch 55/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 452.4175 - val_loss: 526.1409\n",
      "Epoch 56/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 404.3940 - val_loss: 423.4929- ETA: 0s - loss: 399.9\n",
      "Epoch 57/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 427.5402 - val_loss: 973.9061\n",
      "Epoch 58/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 442.4569 - val_loss: 456.3230\n",
      "Epoch 59/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 423.6134 - val_loss: 388.0410\n",
      "Epoch 60/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 395.3441 - val_loss: 768.0545\n",
      "Epoch 61/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 461.1383 - val_loss: 456.6400\n",
      "Epoch 62/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 411.6781 - val_loss: 419.6081\n",
      "Epoch 63/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 408.7274 - val_loss: 535.8331\n",
      "Epoch 64/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 433.6741 - val_loss: 783.8085\n",
      "Epoch 65/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 453.9002 - val_loss: 563.8413\n",
      "Epoch 66/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 466.1215 - val_loss: 415.8548\n",
      "Epoch 67/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 397.5864 - val_loss: 339.9960\n",
      "Epoch 68/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 419.1170 - val_loss: 344.3604\n",
      "Epoch 69/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 426.3800 - val_loss: 368.5026\n",
      "Epoch 70/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 405.8629 - val_loss: 360.5628\n",
      "Epoch 71/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 414.1134 - val_loss: 884.7345\n",
      "Epoch 72/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 407.9299 - val_loss: 342.4401\n",
      "Epoch 73/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 410.0208 - val_loss: 853.3727\n",
      "Epoch 74/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 453.6857 - val_loss: 427.0464\n",
      "Epoch 75/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 402.1250 - val_loss: 417.1595\n",
      "Epoch 76/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 403.9261 - val_loss: 322.5352\n",
      "Epoch 77/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 462.8547 - val_loss: 347.6349\n",
      "Epoch 78/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 398.7783 - val_loss: 417.3005\n",
      "Epoch 79/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 405.2563 - val_loss: 297.2997\n",
      "Epoch 80/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 369.5204 - val_loss: 365.7151\n",
      "Epoch 81/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 382.7764 - val_loss: 287.1206\n",
      "Epoch 82/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 412.2112 - val_loss: 331.2596\n",
      "Epoch 83/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 407.4092 - val_loss: 437.5081\n",
      "Epoch 84/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 409.5452 - val_loss: 410.5166\n",
      "Epoch 85/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 401.9899 - val_loss: 581.6039\n",
      "Epoch 86/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 409.6636 - val_loss: 285.1969\n",
      "Epoch 87/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 432.9566 - val_loss: 537.6660\n",
      "Epoch 88/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 386.6288 - val_loss: 295.0384\n",
      "Epoch 89/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 407.9351 - val_loss: 293.5232\n",
      "Epoch 90/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 378.2030 - val_loss: 382.0597\n",
      "Epoch 91/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 397.6536 - val_loss: 427.7613\n",
      "Epoch 92/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 361.9567 - val_loss: 382.6520\n",
      "Epoch 93/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 419.9495 - val_loss: 310.1094\n",
      "Epoch 94/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 397.0802 - val_loss: 331.4748\n",
      "Epoch 95/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 391.7250 - val_loss: 307.9220\n",
      "Epoch 96/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 333.8601 - val_loss: 652.1750\n",
      "Epoch 97/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 372.6397 - val_loss: 300.4254\n",
      "Epoch 98/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 364.9138 - val_loss: 505.4208\n",
      "Epoch 99/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 346.0035 - val_loss: 603.2674\n",
      "Epoch 100/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 387.4156 - val_loss: 428.2963\n",
      "Epoch 101/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 357.3596 - val_loss: 339.7952\n",
      "Epoch 102/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 350.5901 - val_loss: 555.8671\n",
      "Epoch 103/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 364.3425 - val_loss: 244.1708\n",
      "Epoch 104/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 363.1277 - val_loss: 483.1488\n",
      "Epoch 105/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 326.6912 - val_loss: 321.0838\n",
      "Epoch 106/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 345.9575 - val_loss: 491.5708\n",
      "Epoch 107/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 379.6203 - val_loss: 314.3929\n",
      "Epoch 108/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 338.9488 - val_loss: 276.4310\n",
      "Epoch 109/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 336.7223 - val_loss: 286.4075\n",
      "Epoch 110/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 349.4213 - val_loss: 314.4339\n",
      "Epoch 111/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 321.4991 - val_loss: 233.4199\n",
      "Epoch 112/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 340.6907 - val_loss: 386.2916\n",
      "Epoch 113/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 324.8688 - val_loss: 246.1906\n",
      "Epoch 114/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 350.6113 - val_loss: 278.3943 ETA: 0s - loss: 353.19\n",
      "Epoch 115/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 299.6865 - val_loss: 345.1503\n",
      "Epoch 116/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 328.4623 - val_loss: 434.3516\n",
      "Epoch 117/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 319.1640 - val_loss: 530.0764\n",
      "Epoch 118/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 353.2557 - val_loss: 219.4375TA: 0s - loss: 341.27\n",
      "Epoch 119/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 304.9251 - val_loss: 252.8516\n",
      "Epoch 120/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 338.1313 - val_loss: 447.5719\n",
      "Epoch 121/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 320.3373 - val_loss: 271.2749\n",
      "Epoch 122/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 308.5616 - val_loss: 330.9094\n",
      "Epoch 123/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 322.3253 - val_loss: 401.3632\n",
      "Epoch 124/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 304.4559 - val_loss: 250.8369\n",
      "Epoch 125/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 307.4828 - val_loss: 418.4112\n",
      "Epoch 126/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 316.2599 - val_loss: 404.5909\n",
      "Epoch 127/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 296.7303 - val_loss: 396.7969\n",
      "Epoch 128/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 321.8422 - val_loss: 318.2626\n",
      "Epoch 129/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 300.8836 - val_loss: 254.4892\n",
      "Epoch 130/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 297.1032 - val_loss: 242.5828\n",
      "Epoch 131/1200\n",
      "136/136 [==============================] - ETA: 0s - loss: 315.448 - 1s 4ms/step - loss: 312.5935 - val_loss: 229.4426\n",
      "Epoch 132/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 293.6892 - val_loss: 314.1962\n",
      "Epoch 133/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 290.1398 - val_loss: 203.3576\n",
      "Epoch 134/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 317.6185 - val_loss: 391.8626\n",
      "Epoch 135/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 304.7288 - val_loss: 429.3135\n",
      "Epoch 136/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 325.7106 - val_loss: 240.5968\n",
      "Epoch 137/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 296.1360 - val_loss: 265.8831\n",
      "Epoch 138/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 304.7024 - val_loss: 557.0071\n",
      "Epoch 139/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 295.8510 - val_loss: 214.6561\n",
      "Epoch 140/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 276.7522 - val_loss: 405.9565\n",
      "Epoch 141/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 280.2259 - val_loss: 273.5865\n",
      "Epoch 142/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 307.8483 - val_loss: 262.8077\n",
      "Epoch 143/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 283.2208 - val_loss: 226.1584\n",
      "Epoch 144/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 260.6684 - val_loss: 312.4704\n",
      "Epoch 145/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 287.2014 - val_loss: 237.4321\n",
      "Epoch 146/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 246.1136 - val_loss: 497.4415\n",
      "Epoch 147/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 293.9641 - val_loss: 317.6978\n",
      "Epoch 148/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 274.7064 - val_loss: 302.2402\n",
      "Epoch 149/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 289.6908 - val_loss: 218.0155\n",
      "Epoch 150/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 267.5047 - val_loss: 526.0147\n",
      "Epoch 151/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 1s 4ms/step - loss: 286.6719 - val_loss: 238.1811\n",
      "Epoch 152/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 299.9062 - val_loss: 252.1002\n",
      "Epoch 153/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 257.0453 - val_loss: 243.8453\n",
      "Epoch 154/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 290.6762 - val_loss: 251.5524\n",
      "Epoch 155/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 273.5180 - val_loss: 219.4739\n",
      "Epoch 156/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 295.1144 - val_loss: 177.9868\n",
      "Epoch 157/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 297.2343 - val_loss: 377.9166\n",
      "Epoch 158/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 285.3778 - val_loss: 259.0815\n",
      "Epoch 159/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 263.5905 - val_loss: 286.9595\n",
      "Epoch 160/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 282.4921 - val_loss: 316.9287\n",
      "Epoch 161/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 265.0034 - val_loss: 346.6936\n",
      "Epoch 162/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 272.1779 - val_loss: 213.2567\n",
      "Epoch 163/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 271.2414 - val_loss: 227.3367\n",
      "Epoch 164/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 294.3549 - val_loss: 425.4469\n",
      "Epoch 165/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 271.8234 - val_loss: 215.3721\n",
      "Epoch 166/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 265.0804 - val_loss: 338.9411\n",
      "Epoch 167/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 261.7169 - val_loss: 280.1489\n",
      "Epoch 168/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 264.6616 - val_loss: 279.8680\n",
      "Epoch 169/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 280.1593 - val_loss: 267.6959\n",
      "Epoch 170/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 280.0760 - val_loss: 246.5346\n",
      "Epoch 171/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 267.9441 - val_loss: 460.1382\n",
      "Epoch 172/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 281.8542 - val_loss: 253.3379\n",
      "Epoch 173/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 277.8136 - val_loss: 255.4337\n",
      "Epoch 174/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 261.0424 - val_loss: 195.1782\n",
      "Epoch 175/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 245.1425 - val_loss: 246.8626\n",
      "Epoch 176/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 257.6205 - val_loss: 239.0427\n",
      "Epoch 177/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 281.9707 - val_loss: 278.1610\n",
      "Epoch 178/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 262.0593 - val_loss: 337.2203\n",
      "Epoch 179/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 265.8615 - val_loss: 340.9689\n",
      "Epoch 180/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 247.1187 - val_loss: 306.4553\n",
      "Epoch 181/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 248.2609 - val_loss: 589.7173\n",
      "Epoch 182/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 258.1814 - val_loss: 176.1639\n",
      "Epoch 183/1200\n",
      "136/136 [==============================] - ETA: 0s - loss: 258.626 - 1s 4ms/step - loss: 254.1624 - val_loss: 369.2470\n",
      "Epoch 184/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 271.2988 - val_loss: 321.1708\n",
      "Epoch 185/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 261.4841 - val_loss: 360.5238\n",
      "Epoch 186/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 252.3496 - val_loss: 260.5518\n",
      "Epoch 187/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 246.5524 - val_loss: 256.2586\n",
      "Epoch 188/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 246.9576 - val_loss: 187.9017\n",
      "Epoch 189/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 267.8818 - val_loss: 265.5728\n",
      "Epoch 190/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 253.5739 - val_loss: 291.4526\n",
      "Epoch 191/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 262.0748 - val_loss: 177.6905\n",
      "Epoch 192/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 217.0495 - val_loss: 220.3119\n",
      "Epoch 193/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 222.2818 - val_loss: 225.9129\n",
      "Epoch 194/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 239.5508 - val_loss: 229.1825\n",
      "Epoch 195/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 240.2074 - val_loss: 273.2177\n",
      "Epoch 196/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 227.8918 - val_loss: 276.1211\n",
      "Epoch 197/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 234.6727 - val_loss: 213.8752\n",
      "Epoch 198/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 249.1220 - val_loss: 301.1716\n",
      "Epoch 199/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 253.1286 - val_loss: 212.9887\n",
      "Epoch 200/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 244.7174 - val_loss: 235.3285\n",
      "Epoch 201/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 246.7749 - val_loss: 329.9336\n",
      "Epoch 202/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 242.5123 - val_loss: 292.2291\n",
      "Epoch 203/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 238.5491 - val_loss: 203.8513\n",
      "Epoch 204/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 226.3347 - val_loss: 199.4473\n",
      "Epoch 205/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 233.8578 - val_loss: 154.7186\n",
      "Epoch 206/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 227.5817 - val_loss: 217.4553\n",
      "Epoch 207/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 222.6996 - val_loss: 223.9937\n",
      "Epoch 208/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 237.2558 - val_loss: 191.5163\n",
      "Epoch 209/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 219.2806 - val_loss: 329.2089\n",
      "Epoch 210/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 230.4590 - val_loss: 201.1244\n",
      "Epoch 211/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 234.5331 - val_loss: 424.6436\n",
      "Epoch 212/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 230.5356 - val_loss: 273.3215\n",
      "Epoch 213/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 230.2056 - val_loss: 155.3008\n",
      "Epoch 214/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 217.6076 - val_loss: 256.1832\n",
      "Epoch 215/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 246.1483 - val_loss: 248.5536\n",
      "Epoch 216/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 222.5787 - val_loss: 187.1323\n",
      "Epoch 217/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 242.9198 - val_loss: 190.2830\n",
      "Epoch 218/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 232.4799 - val_loss: 278.0525\n",
      "Epoch 219/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 230.4379 - val_loss: 209.8249\n",
      "Epoch 220/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 230.5761 - val_loss: 195.4486\n",
      "Epoch 221/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 244.7312 - val_loss: 176.9170\n",
      "Epoch 222/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 212.0607 - val_loss: 414.0543\n",
      "Epoch 223/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 210.8272 - val_loss: 190.8729\n",
      "Epoch 224/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 227.6028 - val_loss: 209.7780\n",
      "Epoch 225/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 200.3501 - val_loss: 483.3385\n",
      "Epoch 226/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 223.3679 - val_loss: 230.2347\n",
      "Epoch 227/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 227.0479 - val_loss: 331.2500\n",
      "Epoch 228/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 215.9914 - val_loss: 240.5725\n",
      "Epoch 229/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 221.8313 - val_loss: 143.2547\n",
      "Epoch 230/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 237.4378 - val_loss: 308.1483\n",
      "Epoch 231/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 198.6703 - val_loss: 275.8161\n",
      "Epoch 232/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 198.7892 - val_loss: 240.0690\n",
      "Epoch 233/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 232.6489 - val_loss: 279.8697\n",
      "Epoch 234/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 212.5739 - val_loss: 203.3250\n",
      "Epoch 235/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 229.9673 - val_loss: 538.8625\n",
      "Epoch 236/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 220.1654 - val_loss: 214.9791\n",
      "Epoch 237/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 217.0906 - val_loss: 398.8907\n",
      "Epoch 238/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 209.7095 - val_loss: 195.8840\n",
      "Epoch 239/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 205.1295 - val_loss: 364.0667\n",
      "Epoch 240/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 203.8149 - val_loss: 418.4576\n",
      "Epoch 241/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 227.8853 - val_loss: 270.2968\n",
      "Epoch 242/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 225.3787 - val_loss: 160.0661\n",
      "Epoch 243/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 230.9314 - val_loss: 247.0338\n",
      "Epoch 244/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 213.8435 - val_loss: 272.2412\n",
      "Epoch 245/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 206.4468 - val_loss: 480.8434\n",
      "Epoch 246/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 225.1553 - val_loss: 169.9673\n",
      "Epoch 247/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 210.2555 - val_loss: 396.9997\n",
      "Epoch 248/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 215.7484 - val_loss: 201.1943\n",
      "Epoch 249/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 223.0543 - val_loss: 338.9319\n",
      "Epoch 250/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 224.7170 - val_loss: 142.1462\n",
      "Epoch 251/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 201.8427 - val_loss: 172.4335\n",
      "Epoch 252/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 218.6960 - val_loss: 189.0338\n",
      "Epoch 253/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 220.1630 - val_loss: 162.8816\n",
      "Epoch 254/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 213.4815 - val_loss: 185.9544\n",
      "Epoch 255/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 209.7812 - val_loss: 179.8533\n",
      "Epoch 256/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 175.8765 - val_loss: 323.5901\n",
      "Epoch 257/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 201.3095 - val_loss: 290.5192\n",
      "Epoch 258/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 222.1049 - val_loss: 179.9798\n",
      "Epoch 259/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 208.9994 - val_loss: 311.6274\n",
      "Epoch 260/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 202.7752 - val_loss: 184.4610\n",
      "Epoch 261/1200\n",
      "136/136 [==============================] - ETA: 0s - loss: 193.504 - 1s 5ms/step - loss: 189.4509 - val_loss: 277.1385\n",
      "Epoch 262/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 223.0972 - val_loss: 203.1901\n",
      "Epoch 263/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 191.6165 - val_loss: 152.3342\n",
      "Epoch 264/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 190.3674 - val_loss: 209.5936\n",
      "Epoch 265/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 188.1664 - val_loss: 288.8172\n",
      "Epoch 266/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 204.3947 - val_loss: 353.7305\n",
      "Epoch 267/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 218.0762 - val_loss: 182.6203\n",
      "Epoch 268/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 202.4069 - val_loss: 347.2965\n",
      "Epoch 269/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 203.5555 - val_loss: 276.3631\n",
      "Epoch 270/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 196.2116 - val_loss: 103.9137\n",
      "Epoch 271/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 202.6791 - val_loss: 401.6450\n",
      "Epoch 272/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 220.0113 - val_loss: 121.8686\n",
      "Epoch 273/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 209.5078 - val_loss: 131.3008\n",
      "Epoch 274/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 203.6756 - val_loss: 160.7061\n",
      "Epoch 275/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 194.9265 - val_loss: 299.7519\n",
      "Epoch 276/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 203.8518 - val_loss: 170.0878\n",
      "Epoch 277/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 216.2218 - val_loss: 182.6066\n",
      "Epoch 278/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 196.2859 - val_loss: 152.8924\n",
      "Epoch 279/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 195.7216 - val_loss: 349.8216\n",
      "Epoch 280/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 210.6139 - val_loss: 136.3853\n",
      "Epoch 281/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 196.3628 - val_loss: 181.0981\n",
      "Epoch 282/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 182.2313 - val_loss: 213.8002\n",
      "Epoch 283/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 208.6655 - val_loss: 151.3259\n",
      "Epoch 284/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 204.5698 - val_loss: 117.6488\n",
      "Epoch 285/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 192.4192 - val_loss: 379.8874\n",
      "Epoch 286/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 189.9734 - val_loss: 123.0129\n",
      "Epoch 287/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 217.4259 - val_loss: 205.6735\n",
      "Epoch 288/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 203.3839 - val_loss: 163.3881\n",
      "Epoch 289/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 190.3995 - val_loss: 273.5162\n",
      "Epoch 290/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 196.6961 - val_loss: 182.8967\n",
      "Epoch 291/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 190.6908 - val_loss: 262.2874\n",
      "Epoch 292/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 218.5245 - val_loss: 216.9418\n",
      "Epoch 293/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 175.4213 - val_loss: 174.6747\n",
      "Epoch 294/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 214.1743 - val_loss: 276.0572\n",
      "Epoch 295/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 190.0366 - val_loss: 97.9797\n",
      "Epoch 296/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 179.3723 - val_loss: 173.7764\n",
      "Epoch 297/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 198.2894 - val_loss: 149.9715\n",
      "Epoch 298/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 187.9062 - val_loss: 255.1311\n",
      "Epoch 299/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 202.6155 - val_loss: 260.4320\n",
      "Epoch 300/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 188.9846 - val_loss: 205.6991\n",
      "Epoch 301/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 188.4652 - val_loss: 144.4741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 302/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 185.0831 - val_loss: 224.6884\n",
      "Epoch 303/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 188.3754 - val_loss: 300.3897\n",
      "Epoch 304/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 197.3206 - val_loss: 143.3316\n",
      "Epoch 305/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 185.0966 - val_loss: 215.5118\n",
      "Epoch 306/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 185.0384 - val_loss: 260.9064\n",
      "Epoch 307/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 169.9845 - val_loss: 113.5886\n",
      "Epoch 308/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 209.7820 - val_loss: 150.1638\n",
      "Epoch 309/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 189.8064 - val_loss: 280.9154\n",
      "Epoch 310/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 184.4738 - val_loss: 188.0061\n",
      "Epoch 311/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 186.4433 - val_loss: 167.2136\n",
      "Epoch 312/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 190.3727 - val_loss: 238.5927\n",
      "Epoch 313/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 181.6099 - val_loss: 215.1026\n",
      "Epoch 314/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 185.6441 - val_loss: 184.7301\n",
      "Epoch 315/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 182.2211 - val_loss: 355.6496\n",
      "Epoch 316/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 183.2015 - val_loss: 124.3026\n",
      "Epoch 317/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 183.8949 - val_loss: 221.8985\n",
      "Epoch 318/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 189.5746 - val_loss: 150.7531\n",
      "Epoch 319/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 190.2124 - val_loss: 242.2429\n",
      "Epoch 320/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 183.9395 - val_loss: 87.7210\n",
      "Epoch 321/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 192.1426 - val_loss: 98.5562\n",
      "Epoch 322/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 153.5351 - val_loss: 242.6631\n",
      "Epoch 323/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 186.3617 - val_loss: 348.2550\n",
      "Epoch 324/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 198.4884 - val_loss: 80.9616\n",
      "Epoch 325/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 181.4998 - val_loss: 278.0603\n",
      "Epoch 326/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 183.0293 - val_loss: 161.5757\n",
      "Epoch 327/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 184.5577 - val_loss: 116.8267\n",
      "Epoch 328/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 191.8498 - val_loss: 193.7624\n",
      "Epoch 329/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 185.8995 - val_loss: 147.6433\n",
      "Epoch 330/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 183.0888 - val_loss: 132.3182\n",
      "Epoch 331/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 170.5485 - val_loss: 179.9833\n",
      "Epoch 332/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 175.6021 - val_loss: 80.6999\n",
      "Epoch 333/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 184.0862 - val_loss: 86.1354\n",
      "Epoch 334/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 169.6398 - val_loss: 134.3983\n",
      "Epoch 335/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 172.3153 - val_loss: 187.7575\n",
      "Epoch 336/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 175.8422 - val_loss: 105.7335\n",
      "Epoch 337/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 163.5505 - val_loss: 150.0585\n",
      "Epoch 338/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 197.7159 - val_loss: 172.6606\n",
      "Epoch 339/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 159.9862 - val_loss: 70.7475\n",
      "Epoch 340/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 168.7363 - val_loss: 254.1252\n",
      "Epoch 341/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 183.5262 - val_loss: 110.7146\n",
      "Epoch 342/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 172.4188 - val_loss: 131.6065\n",
      "Epoch 343/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 177.4091 - val_loss: 83.7135\n",
      "Epoch 344/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 179.8827 - val_loss: 200.0237\n",
      "Epoch 345/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 179.3707 - val_loss: 142.9173\n",
      "Epoch 346/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 178.3703 - val_loss: 114.0541\n",
      "Epoch 347/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 161.9361 - val_loss: 244.2226\n",
      "Epoch 348/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 181.5192 - val_loss: 207.4752\n",
      "Epoch 349/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 185.3133 - val_loss: 293.6885\n",
      "Epoch 350/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 176.5689 - val_loss: 72.3851\n",
      "Epoch 351/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 165.8856 - val_loss: 344.0068\n",
      "Epoch 352/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 172.9015 - val_loss: 308.6720\n",
      "Epoch 353/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 179.2847 - val_loss: 108.5337\n",
      "Epoch 354/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 170.7152 - val_loss: 140.2918\n",
      "Epoch 355/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 165.8977 - val_loss: 330.4818\n",
      "Epoch 356/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 179.4991 - val_loss: 147.5964\n",
      "Epoch 357/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 165.5569 - val_loss: 182.9018\n",
      "Epoch 358/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 170.7837 - val_loss: 106.4924\n",
      "Epoch 359/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 170.4872 - val_loss: 189.6435\n",
      "Epoch 360/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 157.7164 - val_loss: 331.5239\n",
      "Epoch 361/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 170.5746 - val_loss: 140.8406\n",
      "Epoch 362/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 170.3775 - val_loss: 210.7528\n",
      "Epoch 363/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 164.9150 - val_loss: 261.7997\n",
      "Epoch 364/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 170.9233 - val_loss: 136.2995\n",
      "Epoch 365/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 181.1946 - val_loss: 262.4332\n",
      "Epoch 366/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 152.3952 - val_loss: 81.2259\n",
      "Epoch 367/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 175.0509 - val_loss: 411.8208\n",
      "Epoch 368/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 183.0459 - val_loss: 257.7881\n",
      "Epoch 369/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 162.4398 - val_loss: 53.4583\n",
      "Epoch 370/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 170.6885 - val_loss: 169.2560\n",
      "Epoch 371/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 178.7269 - val_loss: 115.1767\n",
      "Epoch 372/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 158.2700 - val_loss: 126.1235\n",
      "Epoch 373/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 150.3762 - val_loss: 204.8822\n",
      "Epoch 374/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 165.8206 - val_loss: 174.5239\n",
      "Epoch 375/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 176.3283 - val_loss: 233.8847\n",
      "Epoch 376/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 159.4457 - val_loss: 167.2543\n",
      "Epoch 377/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 170.5058 - val_loss: 186.0975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 378/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 153.8702 - val_loss: 126.8948\n",
      "Epoch 379/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 177.2091 - val_loss: 75.0691\n",
      "Epoch 380/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 163.5453 - val_loss: 151.4366\n",
      "Epoch 381/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 166.7416 - val_loss: 134.2501\n",
      "Epoch 382/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 157.8218 - val_loss: 136.8992\n",
      "Epoch 383/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 168.8380 - val_loss: 273.5447\n",
      "Epoch 384/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 180.1520 - val_loss: 242.4145\n",
      "Epoch 385/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 172.2762 - val_loss: 185.4010\n",
      "Epoch 386/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 158.3731 - val_loss: 134.1457\n",
      "Epoch 387/1200\n",
      "136/136 [==============================] - ETA: 0s - loss: 160.128 - 1s 4ms/step - loss: 158.0297 - val_loss: 152.9205\n",
      "Epoch 388/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 162.0418 - val_loss: 214.7729\n",
      "Epoch 389/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 166.0145 - val_loss: 223.2925\n",
      "Epoch 390/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 162.9444 - val_loss: 97.7854\n",
      "Epoch 391/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 166.5909 - val_loss: 120.3805\n",
      "Epoch 392/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 171.6479 - val_loss: 81.6980\n",
      "Epoch 393/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 160.5672 - val_loss: 225.7116\n",
      "Epoch 394/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 166.7738 - val_loss: 239.2064\n",
      "Epoch 395/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 165.9182 - val_loss: 118.6212\n",
      "Epoch 396/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 153.8033 - val_loss: 158.2268\n",
      "Epoch 397/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 175.4152 - val_loss: 154.9326\n",
      "Epoch 398/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 155.6108 - val_loss: 256.3586\n",
      "Epoch 399/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 170.4533 - val_loss: 258.8221\n",
      "Epoch 400/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 167.5956 - val_loss: 333.3504\n",
      "Epoch 401/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 148.9172 - val_loss: 406.1250\n",
      "Epoch 402/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 166.6937 - val_loss: 111.1062\n",
      "Epoch 403/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 168.0132 - val_loss: 184.3946\n",
      "Epoch 404/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 161.0520 - val_loss: 98.6587\n",
      "Epoch 405/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 172.4790 - val_loss: 96.7993\n",
      "Epoch 406/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 155.7505 - val_loss: 235.4071\n",
      "Epoch 407/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 152.4038 - val_loss: 50.9050\n",
      "Epoch 408/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 172.8863 - val_loss: 274.9569\n",
      "Epoch 409/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 161.7712 - val_loss: 147.6910\n",
      "Epoch 410/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 162.5999 - val_loss: 169.2460\n",
      "Epoch 411/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 161.8884 - val_loss: 62.3377\n",
      "Epoch 412/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 163.3639 - val_loss: 247.4502\n",
      "Epoch 413/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 158.8399 - val_loss: 353.2245\n",
      "Epoch 414/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 167.6666 - val_loss: 128.2545\n",
      "Epoch 415/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 155.0464 - val_loss: 207.0057\n",
      "Epoch 416/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 160.4314 - val_loss: 117.8662\n",
      "Epoch 417/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 163.6967 - val_loss: 193.3868\n",
      "Epoch 418/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 151.6136 - val_loss: 301.2298\n",
      "Epoch 419/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 147.4033 - val_loss: 210.4074\n",
      "Epoch 420/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 166.5835 - val_loss: 178.5799\n",
      "Epoch 421/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 144.6009 - val_loss: 134.2455\n",
      "Epoch 422/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 159.6762 - val_loss: 160.5770\n",
      "Epoch 423/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 155.8860 - val_loss: 241.3973\n",
      "Epoch 424/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 159.4484 - val_loss: 153.6902\n",
      "Epoch 425/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 170.0999 - val_loss: 92.6303\n",
      "Epoch 426/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 138.5713 - val_loss: 277.1949\n",
      "Epoch 427/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 159.7857 - val_loss: 96.8209\n",
      "Epoch 428/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 160.9674 - val_loss: 291.3381\n",
      "Epoch 429/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 154.4415 - val_loss: 203.3220\n",
      "Epoch 430/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 164.7344 - val_loss: 144.6245\n",
      "Epoch 431/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 165.8184 - val_loss: 73.2571\n",
      "Epoch 432/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 176.6240 - val_loss: 195.5373\n",
      "Epoch 433/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 159.3544 - val_loss: 192.7466\n",
      "Epoch 434/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 149.2313 - val_loss: 147.0442\n",
      "Epoch 435/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 159.6904 - val_loss: 279.5562\n",
      "Epoch 436/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 157.2842 - val_loss: 298.2857\n",
      "Epoch 437/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 143.9208 - val_loss: 86.9048\n",
      "Epoch 438/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 155.6575 - val_loss: 350.1480\n",
      "Epoch 439/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 161.4635 - val_loss: 179.7889\n",
      "Epoch 440/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 163.2853 - val_loss: 135.6336\n",
      "Epoch 441/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 152.5793 - val_loss: 130.4149\n",
      "Epoch 442/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 150.9627 - val_loss: 139.4754\n",
      "Epoch 443/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 156.5923 - val_loss: 199.1239\n",
      "Epoch 444/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 154.7733 - val_loss: 127.4774\n",
      "Epoch 445/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 158.4292 - val_loss: 135.1234\n",
      "Epoch 446/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 151.8712 - val_loss: 163.2083\n",
      "Epoch 447/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 160.0755 - val_loss: 264.7431\n",
      "Epoch 448/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 164.1019 - val_loss: 153.4900\n",
      "Epoch 449/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 146.2196 - val_loss: 142.3784\n",
      "Epoch 450/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 163.3477 - val_loss: 242.0139\n",
      "Epoch 451/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 157.8356 - val_loss: 229.9397\n",
      "Epoch 452/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 152.8578 - val_loss: 93.3948\n",
      "Epoch 453/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 161.1901 - val_loss: 104.6884\n",
      "Epoch 454/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 146.3042 - val_loss: 252.4551\n",
      "Epoch 455/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 147.7944 - val_loss: 205.5771\n",
      "Epoch 456/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 153.0070 - val_loss: 162.5753\n",
      "Epoch 457/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 158.0021 - val_loss: 80.9203\n",
      "Epoch 458/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 158.9534 - val_loss: 94.0991\n",
      "Epoch 459/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 132.4302 - val_loss: 161.7172\n",
      "Epoch 460/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 156.7340 - val_loss: 301.6710\n",
      "Epoch 461/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 156.7000 - val_loss: 86.2795\n",
      "Epoch 462/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 151.1850 - val_loss: 290.1763\n",
      "Epoch 463/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 157.9598 - val_loss: 144.7043\n",
      "Epoch 464/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 148.5442 - val_loss: 330.3848\n",
      "Epoch 465/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 143.6293 - val_loss: 214.7893\n",
      "Epoch 466/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 157.6658 - val_loss: 146.2235\n",
      "Epoch 467/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 153.3323 - val_loss: 184.8580\n",
      "Epoch 468/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 150.0969 - val_loss: 221.4619A: 0s - loss: 150.\n",
      "Epoch 469/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 150.9252 - val_loss: 142.3886\n",
      "Epoch 470/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 157.0604 - val_loss: 34.0547\n",
      "Epoch 471/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 128.1756 - val_loss: 81.2358\n",
      "Epoch 472/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 154.1914 - val_loss: 65.82805\n",
      "Epoch 473/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 150.6914 - val_loss: 80.2999\n",
      "Epoch 474/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 139.6588 - val_loss: 208.7724\n",
      "Epoch 475/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 144.2663 - val_loss: 59.8010\n",
      "Epoch 476/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 150.9945 - val_loss: 81.1654\n",
      "Epoch 477/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 144.6769 - val_loss: 219.5756\n",
      "Epoch 478/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 166.8062 - val_loss: 199.8108\n",
      "Epoch 479/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 149.1723 - val_loss: 122.4827\n",
      "Epoch 480/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 146.5983 - val_loss: 105.5952\n",
      "Epoch 481/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 149.3823 - val_loss: 87.5267\n",
      "Epoch 482/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 157.7914 - val_loss: 109.8234\n",
      "Epoch 483/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 147.0051 - val_loss: 57.9893\n",
      "Epoch 484/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 156.5746 - val_loss: 158.9400\n",
      "Epoch 485/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 140.3397 - val_loss: 164.6012\n",
      "Epoch 486/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 155.9068 - val_loss: 178.9712\n",
      "Epoch 487/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 155.9624 - val_loss: 76.9267\n",
      "Epoch 488/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 140.4886 - val_loss: 73.4748\n",
      "Epoch 489/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 160.1271 - val_loss: 73.3296\n",
      "Epoch 490/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 137.0192 - val_loss: 99.9333\n",
      "Epoch 491/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 148.7995 - val_loss: 75.6935\n",
      "Epoch 492/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 144.5076 - val_loss: 165.6391\n",
      "Epoch 493/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 130.9192 - val_loss: 94.8834\n",
      "Epoch 494/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 157.2076 - val_loss: 153.4142\n",
      "Epoch 495/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 154.1274 - val_loss: 128.9774\n",
      "Epoch 496/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 149.4729 - val_loss: 231.2082\n",
      "Epoch 497/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 142.9056 - val_loss: 152.8645\n",
      "Epoch 498/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 155.6826 - val_loss: 178.9945\n",
      "Epoch 499/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 143.1618 - val_loss: 95.1107\n",
      "Epoch 500/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 150.8462 - val_loss: 215.8459\n",
      "Epoch 501/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 148.6062 - val_loss: 133.7970\n",
      "Epoch 502/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 142.1920 - val_loss: 68.9914\n",
      "Epoch 503/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 119.6366 - val_loss: 164.7165\n",
      "Epoch 504/1200\n",
      "136/136 [==============================] - ETA: 0s - loss: 155.845 - 0s 3ms/step - loss: 157.1084 - val_loss: 124.4765\n",
      "Epoch 505/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 144.1767 - val_loss: 76.8316\n",
      "Epoch 506/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 135.9313 - val_loss: 224.5048\n",
      "Epoch 507/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 154.5787 - val_loss: 74.3795\n",
      "Epoch 508/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 151.9157 - val_loss: 349.3585\n",
      "Epoch 509/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 146.9175 - val_loss: 89.2220\n",
      "Epoch 510/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 146.3294 - val_loss: 218.7846\n",
      "Epoch 511/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 134.9497 - val_loss: 181.4112\n",
      "Epoch 512/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 151.4312 - val_loss: 114.4033\n",
      "Epoch 513/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 138.5950 - val_loss: 103.4050\n",
      "Epoch 514/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 130.6502 - val_loss: 184.1478\n",
      "Epoch 515/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 132.6160 - val_loss: 69.1465\n",
      "Epoch 516/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 151.9296 - val_loss: 68.3729\n",
      "Epoch 517/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 146.9295 - val_loss: 88.7393\n",
      "Epoch 518/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 146.2431 - val_loss: 224.1142\n",
      "Epoch 519/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 139.0540 - val_loss: 85.3203\n",
      "Epoch 520/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 146.0817 - val_loss: 110.7966\n",
      "Epoch 521/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 142.6403 - val_loss: 59.8608\n",
      "Epoch 522/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 131.9226 - val_loss: 108.8981\n",
      "Epoch 523/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 149.5714 - val_loss: 146.1858\n",
      "Epoch 524/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 131.9523 - val_loss: 94.6993\n",
      "Epoch 525/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 148.0505 - val_loss: 156.9189\n",
      "Epoch 526/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 142.7076 - val_loss: 134.6972\n",
      "Epoch 527/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 156.3482 - val_loss: 91.4989\n",
      "Epoch 528/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 142.4594 - val_loss: 130.5995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 529/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 133.6510 - val_loss: 164.3197\n",
      "Epoch 530/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 144.9716 - val_loss: 101.6598\n",
      "Epoch 531/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 141.8745 - val_loss: 194.3090\n",
      "Epoch 532/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 131.0836 - val_loss: 128.3805\n",
      "Epoch 533/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 136.2771 - val_loss: 261.3211\n",
      "Epoch 534/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 133.5990 - val_loss: 184.0604\n",
      "Epoch 535/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 141.8875 - val_loss: 160.9204\n",
      "Epoch 536/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 149.6864 - val_loss: 35.5539\n",
      "Epoch 537/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 136.8342 - val_loss: 169.6207\n",
      "Epoch 538/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 145.4595 - val_loss: 215.0847\n",
      "Epoch 539/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 128.4795 - val_loss: 136.5329\n",
      "Epoch 540/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 138.7173 - val_loss: 46.9226\n",
      "Epoch 541/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 146.7679 - val_loss: 36.9685\n",
      "Epoch 542/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 132.9245 - val_loss: 115.8246\n",
      "Epoch 543/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 141.7162 - val_loss: 111.7220\n",
      "Epoch 544/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 132.4484 - val_loss: 124.7156\n",
      "Epoch 545/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 143.2970 - val_loss: 79.2821\n",
      "Epoch 546/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 144.3886 - val_loss: 104.7586\n",
      "Epoch 547/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 118.3663 - val_loss: 116.7131\n",
      "Epoch 548/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 143.4328 - val_loss: 234.7480\n",
      "Epoch 549/1200\n",
      "136/136 [==============================] - ETA: 0s - loss: 142.513 - 0s 3ms/step - loss: 141.7740 - val_loss: 149.9937\n",
      "Epoch 550/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 130.4311 - val_loss: 236.7284\n",
      "Epoch 551/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 138.0514 - val_loss: 112.6540\n",
      "Epoch 552/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 135.7946 - val_loss: 169.9780\n",
      "Epoch 553/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 152.3384 - val_loss: 112.3252\n",
      "Epoch 554/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 156.6892 - val_loss: 132.5650\n",
      "Epoch 555/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 131.4837 - val_loss: 156.9692\n",
      "Epoch 556/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 129.6687 - val_loss: 155.2697\n",
      "Epoch 557/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 135.0430 - val_loss: 134.6056\n",
      "Epoch 558/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 142.2534 - val_loss: 174.3900\n",
      "Epoch 559/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 131.9677 - val_loss: 203.0644\n",
      "Epoch 560/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 139.7895 - val_loss: 179.4772\n",
      "Epoch 561/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 123.1292 - val_loss: 178.1680\n",
      "Epoch 562/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 135.1742 - val_loss: 292.6355\n",
      "Epoch 563/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 139.9541 - val_loss: 86.1533\n",
      "Epoch 564/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 130.9744 - val_loss: 46.8199\n",
      "Epoch 565/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 131.6707 - val_loss: 63.3636\n",
      "Epoch 566/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 135.9572 - val_loss: 207.0234\n",
      "Epoch 567/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 136.6113 - val_loss: 102.0028\n",
      "Epoch 568/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 143.5308 - val_loss: 124.1368\n",
      "Epoch 569/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 137.8903 - val_loss: 86.1712\n",
      "Epoch 570/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 135.9535 - val_loss: 63.1906\n",
      "Epoch 571/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 134.8887 - val_loss: 201.3120\n",
      "Epoch 572/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 126.2299 - val_loss: 127.1511\n",
      "Epoch 573/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 136.5344 - val_loss: 247.9617\n",
      "Epoch 574/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 126.9080 - val_loss: 140.9049\n",
      "Epoch 575/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 121.9193 - val_loss: 272.5380\n",
      "Epoch 576/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 135.1505 - val_loss: 125.1784\n",
      "Epoch 577/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 132.1156 - val_loss: 114.6092\n",
      "Epoch 578/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 130.1904 - val_loss: 141.6526\n",
      "Epoch 579/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 120.8193 - val_loss: 152.7698\n",
      "Epoch 580/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 139.8351 - val_loss: 181.9149\n",
      "Epoch 581/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 122.5309 - val_loss: 91.5354\n",
      "Epoch 582/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 132.2110 - val_loss: 136.2267\n",
      "Epoch 583/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 131.4119 - val_loss: 222.9179\n",
      "Epoch 584/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 126.9462 - val_loss: 175.3593\n",
      "Epoch 585/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 138.9441 - val_loss: 148.8425\n",
      "Epoch 586/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 128.2944 - val_loss: 99.7698\n",
      "Epoch 587/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 135.9130 - val_loss: 123.7511\n",
      "Epoch 588/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 114.1832 - val_loss: 153.2614\n",
      "Epoch 589/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 129.1147 - val_loss: 206.3299\n",
      "Epoch 590/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 129.1888 - val_loss: 85.9380\n",
      "Epoch 591/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 126.5477 - val_loss: 155.2292\n",
      "Epoch 592/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 144.6849 - val_loss: 113.0140\n",
      "Epoch 593/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 127.2858 - val_loss: 199.9710\n",
      "Epoch 594/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 135.8353 - val_loss: 241.6441\n",
      "Epoch 595/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 135.3064 - val_loss: 82.4425\n",
      "Epoch 596/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 134.9778 - val_loss: 64.6378\n",
      "Epoch 597/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 123.9043 - val_loss: 191.5864\n",
      "Epoch 598/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 130.1337 - val_loss: 232.1817\n",
      "Epoch 599/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 125.6736 - val_loss: 253.9141\n",
      "Epoch 600/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 132.4083 - val_loss: 280.1271\n",
      "Epoch 601/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 132.8593 - val_loss: 98.5847\n",
      "Epoch 602/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 126.6469 - val_loss: 88.7246\n",
      "Epoch 603/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 126.1689 - val_loss: 143.9013\n",
      "Epoch 604/1200\n",
      "136/136 [==============================] - 0s 3ms/step - loss: 129.3452 - val_loss: 119.8781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 605/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 130.4203 - val_loss: 191.0584\n",
      "Epoch 606/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 120.8060 - val_loss: 92.9878\n",
      "Epoch 607/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 139.7780 - val_loss: 118.5341\n",
      "Epoch 608/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 121.9814 - val_loss: 108.2321\n",
      "Epoch 609/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 140.5534 - val_loss: 144.8260\n",
      "Epoch 610/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 135.7997 - val_loss: 152.6081\n",
      "Epoch 611/1200\n",
      "136/136 [==============================] - ETA: 0s - loss: 128.809 - 1s 5ms/step - loss: 129.4409 - val_loss: 84.7139\n",
      "Epoch 612/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 125.6570 - val_loss: 263.1883\n",
      "Epoch 613/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 127.5421 - val_loss: 38.9670\n",
      "Epoch 614/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 116.9677 - val_loss: 119.9739\n",
      "Epoch 615/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 140.0559 - val_loss: 150.0246\n",
      "Epoch 616/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 122.0691 - val_loss: 165.1338\n",
      "Epoch 617/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 126.7233 - val_loss: 296.3658\n",
      "Epoch 618/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 130.5708 - val_loss: 113.3074\n",
      "Epoch 619/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 130.6195 - val_loss: 98.7009\n",
      "Epoch 620/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 128.0677 - val_loss: 94.0492\n",
      "Epoch 621/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 123.9729 - val_loss: 103.6803\n",
      "Epoch 622/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 129.5763 - val_loss: 112.7682\n",
      "Epoch 623/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 121.7807 - val_loss: 78.6573\n",
      "Epoch 624/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 128.4479 - val_loss: 246.4989\n",
      "Epoch 625/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 126.9176 - val_loss: 130.5081\n",
      "Epoch 626/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 138.9493 - val_loss: 138.2139\n",
      "Epoch 627/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 122.0664 - val_loss: 129.9855\n",
      "Epoch 628/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 138.7755 - val_loss: 102.7916\n",
      "Epoch 629/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 117.5644 - val_loss: 218.3323\n",
      "Epoch 630/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 136.0704 - val_loss: 59.3777\n",
      "Epoch 631/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 112.7736 - val_loss: 203.1179\n",
      "Epoch 632/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 129.8602 - val_loss: 72.9060\n",
      "Epoch 633/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 128.3382 - val_loss: 77.9949\n",
      "Epoch 634/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 120.3316 - val_loss: 55.9642\n",
      "Epoch 635/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 118.9460 - val_loss: 74.9350\n",
      "Epoch 636/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 132.8306 - val_loss: 172.9085\n",
      "Epoch 637/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 130.5644 - val_loss: 120.9465\n",
      "Epoch 638/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 122.5791 - val_loss: 86.7344\n",
      "Epoch 639/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 121.8484 - val_loss: 92.0286\n",
      "Epoch 640/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 125.5398 - val_loss: 91.5825\n",
      "Epoch 641/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 136.9443 - val_loss: 66.8969\n",
      "Epoch 642/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 119.7311 - val_loss: 108.6736\n",
      "Epoch 643/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 128.2666 - val_loss: 106.5869\n",
      "Epoch 644/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 121.7172 - val_loss: 63.4688\n",
      "Epoch 645/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 120.6112 - val_loss: 88.6576\n",
      "Epoch 646/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 120.8214 - val_loss: 64.4652\n",
      "Epoch 647/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 129.6819 - val_loss: 94.7562\n",
      "Epoch 648/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 133.3796 - val_loss: 115.1856\n",
      "Epoch 649/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 133.7515 - val_loss: 109.6785\n",
      "Epoch 650/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 112.7527 - val_loss: 64.8122\n",
      "Epoch 651/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 135.4156 - val_loss: 160.3753\n",
      "Epoch 652/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 118.0379 - val_loss: 89.5674\n",
      "Epoch 653/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 127.9067 - val_loss: 192.7282\n",
      "Epoch 654/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 124.5616 - val_loss: 112.7900\n",
      "Epoch 655/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 117.1789 - val_loss: 80.4415\n",
      "Epoch 656/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 123.5070 - val_loss: 114.2561\n",
      "Epoch 657/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 109.7477 - val_loss: 123.1756\n",
      "Epoch 658/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 125.6852 - val_loss: 75.3754\n",
      "Epoch 659/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 115.3235 - val_loss: 67.9665\n",
      "Epoch 660/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 128.5968 - val_loss: 177.9782\n",
      "Epoch 661/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 122.5905 - val_loss: 158.6830\n",
      "Epoch 662/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 122.4565 - val_loss: 186.4559\n",
      "Epoch 663/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 124.1321 - val_loss: 154.0683\n",
      "Epoch 664/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 121.6455 - val_loss: 140.7678\n",
      "Epoch 665/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 116.9983 - val_loss: 145.8516\n",
      "Epoch 666/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 124.6470 - val_loss: 68.9643\n",
      "Epoch 667/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 131.4652 - val_loss: 101.2993\n",
      "Epoch 668/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 118.3354 - val_loss: 300.3664\n",
      "Epoch 669/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 129.5869 - val_loss: 86.3159\n",
      "Epoch 670/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 127.2111 - val_loss: 108.0922\n",
      "Epoch 671/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 115.0560 - val_loss: 112.1994\n",
      "Epoch 672/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 117.7458 - val_loss: 50.9562\n",
      "Epoch 673/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 115.9669 - val_loss: 180.3952\n",
      "Epoch 674/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 122.6678 - val_loss: 131.1778\n",
      "Epoch 675/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 127.1409 - val_loss: 116.1886\n",
      "Epoch 676/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 121.7416 - val_loss: 105.6685\n",
      "Epoch 677/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 120.5819 - val_loss: 120.1407\n",
      "Epoch 678/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 117.3781 - val_loss: 43.7835\n",
      "Epoch 679/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 112.9769 - val_loss: 101.4471\n",
      "Epoch 680/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 113.7649 - val_loss: 114.2080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 681/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 116.9030 - val_loss: 191.6043\n",
      "Epoch 682/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 127.9324 - val_loss: 96.1174\n",
      "Epoch 683/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 117.5115 - val_loss: 63.4120\n",
      "Epoch 684/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 120.1957 - val_loss: 109.4740\n",
      "Epoch 685/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 120.1158 - val_loss: 108.2911\n",
      "Epoch 686/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 118.1028 - val_loss: 139.7831\n",
      "Epoch 687/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 118.9980 - val_loss: 95.8246\n",
      "Epoch 688/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 119.9245 - val_loss: 124.5154\n",
      "Epoch 689/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 124.0486 - val_loss: 130.5835\n",
      "Epoch 690/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 120.3607 - val_loss: 50.0707\n",
      "Epoch 691/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 115.3468 - val_loss: 89.0049\n",
      "Epoch 692/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 122.6139 - val_loss: 222.7041\n",
      "Epoch 693/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 127.3548 - val_loss: 109.4263\n",
      "Epoch 694/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 116.0693 - val_loss: 85.8531\n",
      "Epoch 695/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 114.8869 - val_loss: 133.8674\n",
      "Epoch 696/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 121.3611 - val_loss: 154.6388\n",
      "Epoch 697/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 113.6854 - val_loss: 265.9317\n",
      "Epoch 698/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 132.8307 - val_loss: 271.9712\n",
      "Epoch 699/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 118.1272 - val_loss: 51.1118\n",
      "Epoch 700/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 110.3719 - val_loss: 280.4841\n",
      "Epoch 701/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 129.5138 - val_loss: 95.8625\n",
      "Epoch 702/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 117.2041 - val_loss: 124.3680\n",
      "Epoch 703/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 113.1161 - val_loss: 56.4316\n",
      "Epoch 704/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 110.5276 - val_loss: 87.6515\n",
      "Epoch 705/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 124.1394 - val_loss: 136.5353\n",
      "Epoch 706/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 125.0351 - val_loss: 110.7057\n",
      "Epoch 707/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 126.2477 - val_loss: 109.8745\n",
      "Epoch 708/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 121.4702 - val_loss: 88.2651\n",
      "Epoch 709/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 120.2870 - val_loss: 81.2759\n",
      "Epoch 710/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 124.4502 - val_loss: 71.1303\n",
      "Epoch 711/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 116.1848 - val_loss: 117.3256\n",
      "Epoch 712/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 112.6563 - val_loss: 120.6584\n",
      "Epoch 713/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 114.4905 - val_loss: 39.3569\n",
      "Epoch 714/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 108.7253 - val_loss: 100.8518\n",
      "Epoch 715/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 109.0742 - val_loss: 89.0013\n",
      "Epoch 716/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 119.2730 - val_loss: 113.8818\n",
      "Epoch 717/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 121.8450 - val_loss: 86.3473\n",
      "Epoch 718/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 119.0517 - val_loss: 118.0589\n",
      "Epoch 719/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 109.7979 - val_loss: 61.9571\n",
      "Epoch 720/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 115.9139 - val_loss: 128.5354\n",
      "Epoch 721/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 120.4377 - val_loss: 103.4940\n",
      "Epoch 722/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 113.7038 - val_loss: 118.7898\n",
      "Epoch 723/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 114.9807 - val_loss: 149.0577\n",
      "Epoch 724/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 118.2006 - val_loss: 94.8269\n",
      "Epoch 725/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 116.3985 - val_loss: 83.6746\n",
      "Epoch 726/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 120.8856 - val_loss: 155.6542\n",
      "Epoch 727/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 118.5707 - val_loss: 42.1980\n",
      "Epoch 728/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 107.4583 - val_loss: 69.4905\n",
      "Epoch 729/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 98.4458 - val_loss: 145.7957\n",
      "Epoch 730/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 115.2690 - val_loss: 130.8259\n",
      "Epoch 731/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 122.6657 - val_loss: 36.9494\n",
      "Epoch 732/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 107.7669 - val_loss: 40.6792\n",
      "Epoch 733/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 120.5052 - val_loss: 60.4945\n",
      "Epoch 734/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 114.4006 - val_loss: 186.9778\n",
      "Epoch 735/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 123.0165 - val_loss: 87.3621\n",
      "Epoch 736/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 111.2785 - val_loss: 51.0564\n",
      "Epoch 737/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 112.3501 - val_loss: 79.6112\n",
      "Epoch 738/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 111.5050 - val_loss: 55.1856\n",
      "Epoch 739/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 121.5083 - val_loss: 77.9031\n",
      "Epoch 740/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 107.3321 - val_loss: 133.6008 ETA: 0s - loss: 118.5\n",
      "Epoch 741/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 125.8316 - val_loss: 119.7071\n",
      "Epoch 742/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 114.3114 - val_loss: 136.8233\n",
      "Epoch 743/1200\n",
      "136/136 [==============================] - ETA: 0s - loss: 113.8240- ETA: 0s - loss: - 1s 5ms/step - loss: 110.6419 - val_loss: 146.8693\n",
      "Epoch 744/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 113.9151 - val_loss: 62.8024\n",
      "Epoch 745/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 112.1571 - val_loss: 54.5473\n",
      "Epoch 746/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 117.3364 - val_loss: 164.2059\n",
      "Epoch 747/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 104.9101 - val_loss: 161.9090\n",
      "Epoch 748/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 109.0413 - val_loss: 163.9252\n",
      "Epoch 749/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 107.4649 - val_loss: 283.8100\n",
      "Epoch 750/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 120.1905 - val_loss: 145.0729\n",
      "Epoch 751/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 108.4008 - val_loss: 69.1358\n",
      "Epoch 752/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 106.8016 - val_loss: 48.4243\n",
      "Epoch 753/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 107.8372 - val_loss: 67.5838\n",
      "Epoch 754/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 112.8582 - val_loss: 81.8993\n",
      "Epoch 755/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 114.1258 - val_loss: 190.5846\n",
      "Epoch 756/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 112.5878 - val_loss: 89.9951\n",
      "Epoch 757/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 121.6587 - val_loss: 173.4560\n",
      "Epoch 758/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 112.1304 - val_loss: 119.6756\n",
      "Epoch 759/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 106.0504 - val_loss: 75.9377\n",
      "Epoch 760/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 111.7637 - val_loss: 37.8013\n",
      "Epoch 761/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 119.3330 - val_loss: 74.1288\n",
      "Epoch 762/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 117.2619 - val_loss: 57.8103\n",
      "Epoch 763/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 107.3620 - val_loss: 152.2946\n",
      "Epoch 764/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 106.0120 - val_loss: 106.8516\n",
      "Epoch 765/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 110.6019 - val_loss: 173.6947\n",
      "Epoch 766/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 111.0741 - val_loss: 70.7507\n",
      "Epoch 767/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 122.3471 - val_loss: 76.1169\n",
      "Epoch 768/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 103.0464 - val_loss: 191.7278\n",
      "Epoch 769/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 110.3319 - val_loss: 136.7556\n",
      "Epoch 770/1200\n",
      "136/136 [==============================] - 1s 7ms/step - loss: 116.1485 - val_loss: 59.3533\n",
      "Epoch 771/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 107.6374 - val_loss: 189.8288\n",
      "Epoch 772/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 109.4331 - val_loss: 223.7338\n",
      "Epoch 773/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 121.5857 - val_loss: 26.2818\n",
      "Epoch 774/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 110.9598 - val_loss: 101.6676\n",
      "Epoch 775/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 114.8553 - val_loss: 101.5803\n",
      "Epoch 776/1200\n",
      "136/136 [==============================] - 1s 8ms/step - loss: 102.0889 - val_loss: 49.9139\n",
      "Epoch 777/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 112.4619 - val_loss: 115.1635\n",
      "Epoch 778/1200\n",
      "136/136 [==============================] - 1s 7ms/step - loss: 102.1337 - val_loss: 119.4236\n",
      "Epoch 779/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 112.4675 - val_loss: 98.5003\n",
      "Epoch 780/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 108.1228 - val_loss: 117.0700\n",
      "Epoch 781/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 99.5367 - val_loss: 77.8932\n",
      "Epoch 782/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 107.0053 - val_loss: 98.9667\n",
      "Epoch 783/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 99.0931 - val_loss: 206.7055\n",
      "Epoch 784/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 117.6432 - val_loss: 80.8951\n",
      "Epoch 785/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 110.4513 - val_loss: 40.9920\n",
      "Epoch 786/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 97.1173 - val_loss: 55.5807\n",
      "Epoch 787/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 109.8603 - val_loss: 107.3311\n",
      "Epoch 788/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 99.5427 - val_loss: 47.1102\n",
      "Epoch 789/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 107.8819 - val_loss: 100.6627\n",
      "Epoch 790/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 109.8822 - val_loss: 155.9575\n",
      "Epoch 791/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 109.8721 - val_loss: 115.2276\n",
      "Epoch 792/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 116.2954 - val_loss: 80.8464\n",
      "Epoch 793/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 103.9100 - val_loss: 232.3743\n",
      "Epoch 794/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 106.9181 - val_loss: 56.8491\n",
      "Epoch 795/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 99.6956 - val_loss: 176.5866\n",
      "Epoch 796/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 110.5295 - val_loss: 119.3560\n",
      "Epoch 797/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 109.5041 - val_loss: 147.3988\n",
      "Epoch 798/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 110.2419 - val_loss: 165.7736\n",
      "Epoch 799/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 106.8358 - val_loss: 84.7627\n",
      "Epoch 800/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 112.1384 - val_loss: 109.3627\n",
      "Epoch 801/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 111.7438 - val_loss: 106.5516\n",
      "Epoch 802/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 110.2109 - val_loss: 45.8034\n",
      "Epoch 803/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 108.5942 - val_loss: 54.8048\n",
      "Epoch 804/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 110.0615 - val_loss: 111.2071\n",
      "Epoch 805/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 99.5987 - val_loss: 168.7479\n",
      "Epoch 806/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 107.9077 - val_loss: 224.1393\n",
      "Epoch 807/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 116.1566 - val_loss: 143.5740\n",
      "Epoch 808/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 98.3611 - val_loss: 231.2089\n",
      "Epoch 809/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 109.9801 - val_loss: 134.7374\n",
      "Epoch 810/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 103.1206 - val_loss: 56.3728\n",
      "Epoch 811/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 109.7875 - val_loss: 108.6248\n",
      "Epoch 812/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 105.8968 - val_loss: 54.1362\n",
      "Epoch 813/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 110.7757 - val_loss: 136.2057\n",
      "Epoch 814/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 104.1135 - val_loss: 78.8501\n",
      "Epoch 815/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 108.2558 - val_loss: 128.1485\n",
      "Epoch 816/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 96.6629 - val_loss: 113.4327\n",
      "Epoch 817/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 100.0072 - val_loss: 62.3760\n",
      "Epoch 818/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 110.0676 - val_loss: 110.3621\n",
      "Epoch 819/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 113.7509 - val_loss: 139.9952\n",
      "Epoch 820/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 105.2381 - val_loss: 197.5431\n",
      "Epoch 821/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 101.0878 - val_loss: 93.1021\n",
      "Epoch 822/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 102.9140 - val_loss: 132.4161\n",
      "Epoch 823/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 112.0549 - val_loss: 153.7361\n",
      "Epoch 824/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 103.9981 - val_loss: 94.2915\n",
      "Epoch 825/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 101.1792 - val_loss: 173.0930\n",
      "Epoch 826/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 97.8690 - val_loss: 100.8101\n",
      "Epoch 827/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 103.7800 - val_loss: 133.4911\n",
      "Epoch 828/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 110.2407 - val_loss: 76.3658\n",
      "Epoch 829/1200\n",
      "136/136 [==============================] - 0s 4ms/step - loss: 98.6201 - val_loss: 171.8440\n",
      "Epoch 830/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 107.2234 - val_loss: 87.6233\n",
      "Epoch 831/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 111.6980 - val_loss: 74.3005\n",
      "Epoch 832/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 1s 4ms/step - loss: 100.4426 - val_loss: 202.0176\n",
      "Epoch 833/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 113.0483 - val_loss: 143.8874\n",
      "Epoch 834/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 92.7664 - val_loss: 186.3575\n",
      "Epoch 835/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 107.7374 - val_loss: 152.3774\n",
      "Epoch 836/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 100.1562 - val_loss: 56.8469\n",
      "Epoch 837/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 103.8566 - val_loss: 139.9984\n",
      "Epoch 838/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 96.2654 - val_loss: 70.4368\n",
      "Epoch 839/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 103.1239 - val_loss: 84.4448\n",
      "Epoch 840/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 104.5963 - val_loss: 28.6029\n",
      "Epoch 841/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 100.5399 - val_loss: 127.2191\n",
      "Epoch 842/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 104.3864 - val_loss: 156.3298\n",
      "Epoch 843/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 105.5775 - val_loss: 70.8058\n",
      "Epoch 844/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 103.0936 - val_loss: 58.8503\n",
      "Epoch 845/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 102.1416 - val_loss: 194.8824\n",
      "Epoch 846/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 104.4900 - val_loss: 105.6094\n",
      "Epoch 847/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 92.8964 - val_loss: 74.1083\n",
      "Epoch 848/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 139.4190 - val_loss: 167.1766\n",
      "Epoch 849/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 103.9074 - val_loss: 167.0637\n",
      "Epoch 850/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 98.3227 - val_loss: 105.0716\n",
      "Epoch 851/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 101.6135 - val_loss: 149.3934\n",
      "Epoch 852/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 103.0734 - val_loss: 186.5085\n",
      "Epoch 853/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 99.6123 - val_loss: 89.7613\n",
      "Epoch 854/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 104.1642 - val_loss: 67.6829\n",
      "Epoch 855/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 98.5629 - val_loss: 77.4790\n",
      "Epoch 856/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 101.5527 - val_loss: 149.2789\n",
      "Epoch 857/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 108.1301 - val_loss: 76.6935\n",
      "Epoch 858/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 104.1971 - val_loss: 84.17119\n",
      "Epoch 859/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 111.8568 - val_loss: 38.1575\n",
      "Epoch 860/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 141.7172 - val_loss: 59.2762\n",
      "Epoch 861/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 99.2778 - val_loss: 180.6346\n",
      "Epoch 862/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 101.8295 - val_loss: 96.3859\n",
      "Epoch 863/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 103.1345 - val_loss: 46.7540\n",
      "Epoch 864/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 106.6388 - val_loss: 56.1677\n",
      "Epoch 865/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 98.0475 - val_loss: 68.4568\n",
      "Epoch 866/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 90.2745 - val_loss: 76.2733\n",
      "Epoch 867/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 106.0563 - val_loss: 57.1402\n",
      "Epoch 868/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 88.4045 - val_loss: 133.1748\n",
      "Epoch 869/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 101.1563 - val_loss: 78.3705\n",
      "Epoch 870/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 105.5376 - val_loss: 90.7663\n",
      "Epoch 871/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 101.4473 - val_loss: 124.4477\n",
      "Epoch 872/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 92.5241 - val_loss: 69.2531\n",
      "Epoch 873/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 109.9911 - val_loss: 100.4837\n",
      "Epoch 874/1200\n",
      "136/136 [==============================] - ETA: 0s - loss: 99.89 - 1s 4ms/step - loss: 98.7084 - val_loss: 109.7313\n",
      "Epoch 875/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 100.0448 - val_loss: 82.3480\n",
      "Epoch 876/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 100.8628 - val_loss: 37.0748\n",
      "Epoch 877/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 100.5493 - val_loss: 60.9467\n",
      "Epoch 878/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 99.4390 - val_loss: 90.7768\n",
      "Epoch 879/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 97.0809 - val_loss: 102.1914\n",
      "Epoch 880/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 101.7527 - val_loss: 127.0768\n",
      "Epoch 881/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 97.6891 - val_loss: 66.0937\n",
      "Epoch 882/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 98.5481 - val_loss: 105.1049\n",
      "Epoch 883/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 98.5847 - val_loss: 120.4233\n",
      "Epoch 884/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 101.3237 - val_loss: 122.1853\n",
      "Epoch 885/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 104.8208 - val_loss: 75.4644\n",
      "Epoch 886/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 101.3449 - val_loss: 114.6686\n",
      "Epoch 887/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 99.4063 - val_loss: 50.0763\n",
      "Epoch 888/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 96.7790 - val_loss: 83.5918\n",
      "Epoch 889/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 97.8199 - val_loss: 115.1916\n",
      "Epoch 890/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 102.3530 - val_loss: 80.4429\n",
      "Epoch 891/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 91.5383 - val_loss: 140.3327\n",
      "Epoch 892/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 98.0602 - val_loss: 159.9648\n",
      "Epoch 893/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 91.0165 - val_loss: 128.4177\n",
      "Epoch 894/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 102.2361 - val_loss: 85.4078\n",
      "Epoch 895/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 100.8220 - val_loss: 106.1260\n",
      "Epoch 896/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 99.8418 - val_loss: 104.2857\n",
      "Epoch 897/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 91.7285 - val_loss: 97.2003\n",
      "Epoch 898/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 102.2695 - val_loss: 96.4639\n",
      "Epoch 899/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 94.9312 - val_loss: 217.7961\n",
      "Epoch 900/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 97.6398 - val_loss: 86.3844\n",
      "Epoch 901/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 92.3519 - val_loss: 161.6476\n",
      "Epoch 902/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 105.8095 - val_loss: 152.5067\n",
      "Epoch 903/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 95.1004 - val_loss: 72.7974\n",
      "Epoch 904/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 107.6888 - val_loss: 55.0057\n",
      "Epoch 905/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 93.4427 - val_loss: 114.4820\n",
      "Epoch 906/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 98.0058 - val_loss: 270.0636\n",
      "Epoch 907/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 111.1632 - val_loss: 53.9609\n",
      "Epoch 908/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 90.1301 - val_loss: 30.0905\n",
      "Epoch 909/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 93.8498 - val_loss: 62.9565\n",
      "Epoch 910/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 216.1215 - val_loss: 90.5744\n",
      "Epoch 911/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 210.1575 - val_loss: 92.1629\n",
      "Epoch 912/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 96.2448 - val_loss: 58.4245\n",
      "Epoch 913/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 90.2902 - val_loss: 80.4395\n",
      "Epoch 914/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 127.1156 - val_loss: 123.7949\n",
      "Epoch 915/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 100.5808 - val_loss: 40.4845\n",
      "Epoch 916/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 98.3213 - val_loss: 86.3720\n",
      "Epoch 917/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 99.5832 - val_loss: 208.3624\n",
      "Epoch 918/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 101.5229 - val_loss: 143.6829\n",
      "Epoch 919/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 106.1215 - val_loss: 59.6753\n",
      "Epoch 920/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 98.0529 - val_loss: 122.8125\n",
      "Epoch 921/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 93.7432 - val_loss: 153.3078\n",
      "Epoch 922/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 97.2640 - val_loss: 67.6423\n",
      "Epoch 923/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 93.3698 - val_loss: 106.7967\n",
      "Epoch 924/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 97.1855 - val_loss: 212.0101\n",
      "Epoch 925/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 98.8299 - val_loss: 101.9904\n",
      "Epoch 926/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 95.4876 - val_loss: 151.6033\n",
      "Epoch 927/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 92.2476 - val_loss: 76.8733\n",
      "Epoch 928/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 101.1613 - val_loss: 191.1195\n",
      "Epoch 929/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 93.0521 - val_loss: 126.6328\n",
      "Epoch 930/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 98.9447 - val_loss: 83.9883\n",
      "Epoch 931/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 101.5112 - val_loss: 52.6632\n",
      "Epoch 932/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 100.0291 - val_loss: 115.6871\n",
      "Epoch 933/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 90.5846 - val_loss: 89.1234 ETA: 0s - l\n",
      "Epoch 934/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 97.9751 - val_loss: 47.4194\n",
      "Epoch 935/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 96.7360 - val_loss: 95.0715\n",
      "Epoch 936/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 95.2061 - val_loss: 98.2826\n",
      "Epoch 937/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 92.8630 - val_loss: 140.4139\n",
      "Epoch 938/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 98.7632 - val_loss: 73.8659\n",
      "Epoch 939/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 86.8834 - val_loss: 71.9167\n",
      "Epoch 940/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 122.5488 - val_loss: 54.6785\n",
      "Epoch 941/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 87.7539 - val_loss: 90.8287\n",
      "Epoch 942/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 77.6868 - val_loss: 86.6117\n",
      "Epoch 943/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 87.4114 - val_loss: 109.7549\n",
      "Epoch 944/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 94.9028 - val_loss: 108.4013\n",
      "Epoch 945/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 97.8096 - val_loss: 106.1684\n",
      "Epoch 946/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 122.9483 - val_loss: 97.1980 ETA: 0s - loss: 125.039\n",
      "Epoch 947/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 87.5967 - val_loss: 120.6822\n",
      "Epoch 948/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 88.9418 - val_loss: 72.0345\n",
      "Epoch 949/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 90.7881 - val_loss: 68.2505\n",
      "Epoch 950/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 102.4881 - val_loss: 90.0560\n",
      "Epoch 951/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 93.4075 - val_loss: 100.1877\n",
      "Epoch 952/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 90.0555 - val_loss: 82.0742\n",
      "Epoch 953/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 93.5552 - val_loss: 43.0848\n",
      "Epoch 954/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 88.3939 - val_loss: 51.2222\n",
      "Epoch 955/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 95.6657 - val_loss: 182.9981\n",
      "Epoch 956/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 154.6948 - val_loss: 52.7588\n",
      "Epoch 957/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 85.6475 - val_loss: 43.1860\n",
      "Epoch 958/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 81.6983 - val_loss: 118.2544\n",
      "Epoch 959/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 92.5340 - val_loss: 105.2687\n",
      "Epoch 960/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 99.8100 - val_loss: 103.2316\n",
      "Epoch 961/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 85.3911 - val_loss: 209.5201\n",
      "Epoch 962/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 99.4938 - val_loss: 41.9483\n",
      "Epoch 963/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 90.6017 - val_loss: 53.6772\n",
      "Epoch 964/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 91.8124 - val_loss: 109.6501\n",
      "Epoch 965/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 103.8718 - val_loss: 113.7462\n",
      "Epoch 966/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 87.9978 - val_loss: 65.1694\n",
      "Epoch 967/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 95.2753 - val_loss: 114.5843\n",
      "Epoch 968/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 104.1098 - val_loss: 79.2970\n",
      "Epoch 969/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 87.4898 - val_loss: 139.9533\n",
      "Epoch 970/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 124.1911 - val_loss: 55.0979\n",
      "Epoch 971/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 263.5413 - val_loss: 74.76360\n",
      "Epoch 972/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 374.7890 - val_loss: 79.7653\n",
      "Epoch 973/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 335.8262 - val_loss: 130.5726\n",
      "Epoch 974/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 378.5776 - val_loss: 100.3963\n",
      "Epoch 975/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 256.5696 - val_loss: 110.1339\n",
      "Epoch 976/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 411.4611 - val_loss: 217.4017\n",
      "Epoch 977/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 473.0921 - val_loss: 75.2541\n",
      "Epoch 978/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 93.1273 - val_loss: 116.3173\n",
      "Epoch 979/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 83.9953 - val_loss: 65.5796\n",
      "Epoch 980/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 92.3036 - val_loss: 58.4831\n",
      "Epoch 981/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 89.2428 - val_loss: 162.3809\n",
      "Epoch 982/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 137.6086 - val_loss: 182.7323\n",
      "Epoch 983/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 96.5338 - val_loss: 99.4654\n",
      "Epoch 984/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 1s 5ms/step - loss: 84.5094 - val_loss: 142.9691\n",
      "Epoch 985/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 102.2648 - val_loss: 146.1608\n",
      "Epoch 986/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 91.5949 - val_loss: 97.8504\n",
      "Epoch 987/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 95.6530 - val_loss: 70.6633\n",
      "Epoch 988/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 94.6740 - val_loss: 75.8908\n",
      "Epoch 989/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 93.6216 - val_loss: 64.4053\n",
      "Epoch 990/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 87.8814 - val_loss: 98.5617\n",
      "Epoch 991/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 92.4173 - val_loss: 114.6027\n",
      "Epoch 992/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 86.6274 - val_loss: 53.4202\n",
      "Epoch 993/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 94.6519 - val_loss: 106.3646\n",
      "Epoch 994/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 98.9500 - val_loss: 76.6069\n",
      "Epoch 995/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 90.6828 - val_loss: 62.5231\n",
      "Epoch 996/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 88.8490 - val_loss: 44.8902\n",
      "Epoch 997/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 96.6975 - val_loss: 63.3110\n",
      "Epoch 998/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 95.9865 - val_loss: 55.5280\n",
      "Epoch 999/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 90.0967 - val_loss: 51.7462\n",
      "Epoch 1000/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 99.1839 - val_loss: 141.2686\n",
      "Epoch 1001/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 133.3305 - val_loss: 126.6934\n",
      "Epoch 1002/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 87.2774 - val_loss: 69.7645\n",
      "Epoch 1003/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 98.0327 - val_loss: 74.7430\n",
      "Epoch 1004/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 76.9715 - val_loss: 94.5986\n",
      "Epoch 1005/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 91.8037 - val_loss: 185.6197\n",
      "Epoch 1006/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 97.7175 - val_loss: 58.8115\n",
      "Epoch 1007/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 91.8516 - val_loss: 97.0294\n",
      "Epoch 1008/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 92.0433 - val_loss: 118.5021\n",
      "Epoch 1009/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 91.0666 - val_loss: 69.7081\n",
      "Epoch 1010/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 93.8210 - val_loss: 43.7164\n",
      "Epoch 1011/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 88.1915 - val_loss: 108.9847\n",
      "Epoch 1012/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 89.4372 - val_loss: 59.6505\n",
      "Epoch 1013/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 80.3882 - val_loss: 81.3251\n",
      "Epoch 1014/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 87.9103 - val_loss: 58.7289\n",
      "Epoch 1015/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 84.3341 - val_loss: 97.5059\n",
      "Epoch 1016/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 93.7398 - val_loss: 132.5346\n",
      "Epoch 1017/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 84.8389 - val_loss: 68.6426\n",
      "Epoch 1018/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 125.4723 - val_loss: 80.8543\n",
      "Epoch 1019/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 92.1987 - val_loss: 34.1705\n",
      "Epoch 1020/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 88.1259 - val_loss: 67.5197\n",
      "Epoch 1021/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 91.0686 - val_loss: 72.1894\n",
      "Epoch 1022/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 86.3799 - val_loss: 59.3987\n",
      "Epoch 1023/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 96.0766 - val_loss: 70.6349\n",
      "Epoch 1024/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 92.2351 - val_loss: 96.0562\n",
      "Epoch 1025/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 90.1921 - val_loss: 77.0352\n",
      "Epoch 1026/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 88.8879 - val_loss: 37.5181\n",
      "Epoch 1027/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 82.8599 - val_loss: 101.6320\n",
      "Epoch 1028/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 87.6615 - val_loss: 105.4083\n",
      "Epoch 1029/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 85.7270 - val_loss: 55.8014\n",
      "Epoch 1030/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 83.5482 - val_loss: 138.2704\n",
      "Epoch 1031/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 86.3547 - val_loss: 146.4524\n",
      "Epoch 1032/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 89.9872 - val_loss: 74.5670\n",
      "Epoch 1033/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 95.3313 - val_loss: 72.2190\n",
      "Epoch 1034/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 93.5894 - val_loss: 98.2408\n",
      "Epoch 1035/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 90.3713 - val_loss: 60.1612\n",
      "Epoch 1036/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 92.9582 - val_loss: 32.8183\n",
      "Epoch 1037/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 88.6746 - val_loss: 34.4161\n",
      "Epoch 1038/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 91.9314 - val_loss: 80.9865\n",
      "Epoch 1039/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 123.7508 - val_loss: 55.9342\n",
      "Epoch 1040/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 89.8193 - val_loss: 66.3567\n",
      "Epoch 1041/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 89.8879 - val_loss: 89.9883\n",
      "Epoch 1042/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 98.0866 - val_loss: 141.4053\n",
      "Epoch 1043/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 85.5374 - val_loss: 79.9560\n",
      "Epoch 1044/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 86.8916 - val_loss: 63.5862\n",
      "Epoch 1045/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 128.9062 - val_loss: 91.8993\n",
      "Epoch 1046/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 91.5312 - val_loss: 78.9484\n",
      "Epoch 1047/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 78.1805 - val_loss: 45.9051\n",
      "Epoch 1048/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 90.1868 - val_loss: 58.7157\n",
      "Epoch 1049/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 84.5731 - val_loss: 159.3291\n",
      "Epoch 1050/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 89.8606 - val_loss: 78.6996\n",
      "Epoch 1051/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 84.3551 - val_loss: 221.9628\n",
      "Epoch 1052/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 95.8567 - val_loss: 50.6002\n",
      "Epoch 1053/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 88.2769 - val_loss: 112.6429\n",
      "Epoch 1054/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 88.7193 - val_loss: 83.2113\n",
      "Epoch 1055/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 85.9835 - val_loss: 151.1450\n",
      "Epoch 1056/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 79.0538 - val_loss: 139.3121\n",
      "Epoch 1057/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 89.7210 - val_loss: 75.1933\n",
      "Epoch 1058/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 91.5976 - val_loss: 101.0559\n",
      "Epoch 1059/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 86.6116 - val_loss: 40.5304\n",
      "Epoch 1060/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 82.4755 - val_loss: 136.9303\n",
      "Epoch 1061/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 85.9047 - val_loss: 70.7870\n",
      "Epoch 1062/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 86.2225 - val_loss: 118.9711\n",
      "Epoch 1063/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 84.0707 - val_loss: 60.0943\n",
      "Epoch 1064/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 82.2318 - val_loss: 66.5908\n",
      "Epoch 1065/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 94.2583 - val_loss: 76.2810\n",
      "Epoch 1066/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 82.6940 - val_loss: 90.3876\n",
      "Epoch 1067/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 86.0337 - val_loss: 76.9392\n",
      "Epoch 1068/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 91.0361 - val_loss: 87.2874\n",
      "Epoch 1069/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 88.5820 - val_loss: 93.4365\n",
      "Epoch 1070/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 81.7321 - val_loss: 30.2022\n",
      "Epoch 1071/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 81.5949 - val_loss: 68.3027\n",
      "Epoch 1072/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 85.6863 - val_loss: 127.6725\n",
      "Epoch 1073/1200\n",
      "136/136 [==============================] - ETA: 0s - loss: 83.38 - 1s 5ms/step - loss: 84.6689 - val_loss: 78.9951\n",
      "Epoch 1074/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 83.7767 - val_loss: 190.9802\n",
      "Epoch 1075/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 88.7975 - val_loss: 38.0232\n",
      "Epoch 1076/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 73.5436 - val_loss: 60.8042\n",
      "Epoch 1077/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 94.4738 - val_loss: 91.3844\n",
      "Epoch 1078/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 82.1844 - val_loss: 96.8491\n",
      "Epoch 1079/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 85.7564 - val_loss: 62.5333\n",
      "Epoch 1080/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 86.6748 - val_loss: 102.1094\n",
      "Epoch 1081/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 70.9029 - val_loss: 89.0184\n",
      "Epoch 1082/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 92.2498 - val_loss: 150.9241\n",
      "Epoch 1083/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 91.1465 - val_loss: 44.6424\n",
      "Epoch 1084/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 77.4284 - val_loss: 63.6105\n",
      "Epoch 1085/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 84.0198 - val_loss: 140.5758\n",
      "Epoch 1086/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 88.9909 - val_loss: 97.8272\n",
      "Epoch 1087/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 77.4898 - val_loss: 124.7522\n",
      "Epoch 1088/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 87.6250 - val_loss: 75.6749\n",
      "Epoch 1089/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 88.5201 - val_loss: 114.2492\n",
      "Epoch 1090/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 85.5007 - val_loss: 63.1557\n",
      "Epoch 1091/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 89.7597 - val_loss: 51.8055\n",
      "Epoch 1092/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 89.5661 - val_loss: 113.4133\n",
      "Epoch 1093/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 89.7735 - val_loss: 88.0657\n",
      "Epoch 1094/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 82.0565 - val_loss: 53.9841\n",
      "Epoch 1095/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 78.4586 - val_loss: 66.4421\n",
      "Epoch 1096/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 82.5091 - val_loss: 106.8801\n",
      "Epoch 1097/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 84.9067 - val_loss: 74.5183\n",
      "Epoch 1098/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 87.6883 - val_loss: 65.5547\n",
      "Epoch 1099/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 87.5712 - val_loss: 55.0857\n",
      "Epoch 1100/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 79.2645 - val_loss: 114.3375\n",
      "Epoch 1101/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 81.9121 - val_loss: 124.7395\n",
      "Epoch 1102/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 80.0531 - val_loss: 130.1088\n",
      "Epoch 1103/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 88.1981 - val_loss: 73.2321\n",
      "Epoch 1104/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 91.0303 - val_loss: 80.6847\n",
      "Epoch 1105/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 72.6194 - val_loss: 92.8639\n",
      "Epoch 1106/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 84.3736 - val_loss: 96.8576\n",
      "Epoch 1107/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 78.8599 - val_loss: 190.6742\n",
      "Epoch 1108/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 85.1310 - val_loss: 97.8488\n",
      "Epoch 1109/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 129.2640 - val_loss: 107.7073\n",
      "Epoch 1110/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 123.8970 - val_loss: 119.2126\n",
      "Epoch 1111/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 80.9929 - val_loss: 159.1404\n",
      "Epoch 1112/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 81.5199 - val_loss: 118.7394\n",
      "Epoch 1113/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 84.3781 - val_loss: 92.7838\n",
      "Epoch 1114/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 89.7949 - val_loss: 119.5743\n",
      "Epoch 1115/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 90.2269 - val_loss: 37.1909\n",
      "Epoch 1116/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 85.3179 - val_loss: 123.7765\n",
      "Epoch 1117/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 76.7194 - val_loss: 63.1755\n",
      "Epoch 1118/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 79.9698 - val_loss: 91.5765\n",
      "Epoch 1119/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 82.0109 - val_loss: 46.1972\n",
      "Epoch 1120/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 85.5134 - val_loss: 70.3689\n",
      "Epoch 1121/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 79.5766 - val_loss: 68.1195\n",
      "Epoch 1122/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 85.6470 - val_loss: 100.5884\n",
      "Epoch 1123/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 82.5857 - val_loss: 93.8577\n",
      "Epoch 1124/1200\n",
      "136/136 [==============================] - 1s 7ms/step - loss: 84.0109 - val_loss: 56.1229\n",
      "Epoch 1125/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 80.5252 - val_loss: 46.3862\n",
      "Epoch 1126/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 90.2139 - val_loss: 122.8228\n",
      "Epoch 1127/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 87.0635 - val_loss: 81.6395TA: 0s - loss: 86.04\n",
      "Epoch 1128/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 84.5674 - val_loss: 106.8440\n",
      "Epoch 1129/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 82.3633 - val_loss: 100.0922\n",
      "Epoch 1130/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 126.3691 - val_loss: 68.1508\n",
      "Epoch 1131/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 121.7510 - val_loss: 144.9914\n",
      "Epoch 1132/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 84.1785 - val_loss: 51.3154\n",
      "Epoch 1133/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 121.9737 - val_loss: 122.0734\n",
      "Epoch 1134/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 88.5252 - val_loss: 84.7813\n",
      "Epoch 1135/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 82.6146 - val_loss: 92.7285\n",
      "Epoch 1136/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136/136 [==============================] - 1s 6ms/step - loss: 86.2931 - val_loss: 113.2306\n",
      "Epoch 1137/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 81.6828 - val_loss: 66.4683\n",
      "Epoch 1138/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 83.6612 - val_loss: 124.3552\n",
      "Epoch 1139/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 84.6309 - val_loss: 147.7665\n",
      "Epoch 1140/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 90.1351 - val_loss: 40.3145\n",
      "Epoch 1141/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 85.7687 - val_loss: 68.3929\n",
      "Epoch 1142/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 83.6171 - val_loss: 123.3042\n",
      "Epoch 1143/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 81.0721 - val_loss: 115.7693\n",
      "Epoch 1144/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 83.6740 - val_loss: 47.5532\n",
      "Epoch 1145/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 85.8853 - val_loss: 86.4047\n",
      "Epoch 1146/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 80.5900 - val_loss: 147.4710\n",
      "Epoch 1147/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 89.4574 - val_loss: 119.1676\n",
      "Epoch 1148/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 78.8308 - val_loss: 48.5554\n",
      "Epoch 1149/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 80.1447 - val_loss: 39.1423\n",
      "Epoch 1150/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 87.6703 - val_loss: 51.1568\n",
      "Epoch 1151/1200\n",
      "136/136 [==============================] - ETA: 0s - loss: 83.91 - 1s 5ms/step - loss: 83.2520 - val_loss: 43.2863\n",
      "Epoch 1152/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 78.6836 - val_loss: 129.2532\n",
      "Epoch 1153/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 79.1085 - val_loss: 86.6711\n",
      "Epoch 1154/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 84.3032 - val_loss: 62.1634\n",
      "Epoch 1155/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 79.8367 - val_loss: 25.6075\n",
      "Epoch 1156/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 126.8882 - val_loss: 103.5479\n",
      "Epoch 1157/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 84.7695 - val_loss: 79.9279\n",
      "Epoch 1158/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 78.4291 - val_loss: 93.0171\n",
      "Epoch 1159/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 87.7319 - val_loss: 127.1128\n",
      "Epoch 1160/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 80.9919 - val_loss: 52.4365\n",
      "Epoch 1161/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 84.1453 - val_loss: 49.3374\n",
      "Epoch 1162/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 86.7936 - val_loss: 55.2256\n",
      "Epoch 1163/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 78.2887 - val_loss: 70.3366\n",
      "Epoch 1164/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 76.2575 - val_loss: 121.8844\n",
      "Epoch 1165/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 89.4124 - val_loss: 43.7836\n",
      "Epoch 1166/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 77.6552 - val_loss: 140.9320\n",
      "Epoch 1167/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 83.2169 - val_loss: 76.2582\n",
      "Epoch 1168/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 89.3351 - val_loss: 99.4824\n",
      "Epoch 1169/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 84.1971 - val_loss: 48.4206\n",
      "Epoch 1170/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 79.1411 - val_loss: 90.5854\n",
      "Epoch 1171/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 86.6547 - val_loss: 68.1600\n",
      "Epoch 1172/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 84.3838 - val_loss: 42.3631\n",
      "Epoch 1173/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 76.8985 - val_loss: 132.6781\n",
      "Epoch 1174/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 76.5948 - val_loss: 63.1824\n",
      "Epoch 1175/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 76.1524 - val_loss: 37.9295\n",
      "Epoch 1176/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 204.3025 - val_loss: 84.7110\n",
      "Epoch 1177/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 94.1019 - val_loss: 113.0471\n",
      "Epoch 1178/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 73.3718 - val_loss: 121.9731\n",
      "Epoch 1179/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 76.5892 - val_loss: 122.2815\n",
      "Epoch 1180/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 82.8833 - val_loss: 103.1851\n",
      "Epoch 1181/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 82.4225 - val_loss: 77.9322\n",
      "Epoch 1182/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 79.9381 - val_loss: 51.4099\n",
      "Epoch 1183/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 81.1383 - val_loss: 65.3631\n",
      "Epoch 1184/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 80.5691 - val_loss: 41.8013\n",
      "Epoch 1185/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 69.3902 - val_loss: 22.3204\n",
      "Epoch 1186/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 84.5828 - val_loss: 94.5911\n",
      "Epoch 1187/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 73.2598 - val_loss: 69.0033\n",
      "Epoch 1188/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 84.0889 - val_loss: 131.3890\n",
      "Epoch 1189/1200\n",
      "136/136 [==============================] - 1s 6ms/step - loss: 84.9370 - val_loss: 75.8608\n",
      "Epoch 1190/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 85.8964 - val_loss: 103.3784\n",
      "Epoch 1191/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 78.4357 - val_loss: 131.6690\n",
      "Epoch 1192/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 123.8229 - val_loss: 66.1548\n",
      "Epoch 1193/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 81.9462 - val_loss: 84.2750\n",
      "Epoch 1194/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 89.2871 - val_loss: 94.7464\n",
      "Epoch 1195/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 74.5456 - val_loss: 95.5894\n",
      "Epoch 1196/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 71.9227 - val_loss: 53.0891\n",
      "Epoch 1197/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 80.4073 - val_loss: 124.0405\n",
      "Epoch 1198/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 80.2445 - val_loss: 102.1072\n",
      "Epoch 1199/1200\n",
      "136/136 [==============================] - 1s 5ms/step - loss: 85.0628 - val_loss: 46.2729\n",
      "Epoch 1200/1200\n",
      "136/136 [==============================] - 1s 4ms/step - loss: 73.5727 - val_loss: 71.2675\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train, batch_size=4, epochs=1200, validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(currentFile/modelNum, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(currentFile/weightsNum)\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "arr = []\n",
    "for x in range(len(X_scale)):\n",
    "    Xnew = array([[X_scale[x][0], X_scale[x][1] , X_scale[x][2], X_scale[x][3], X_scale[x][4], X_scale[x][5]]])\n",
    "    ynew = model.predict(Xnew)\n",
    "    #print(ynew[0][0])\n",
    "    arr.append(ynew[0][0])\n",
    "array = np.array(arr)\n",
    "print(array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.982887195057358\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(Y.tolist(), array.tolist())\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_error(actual, predicted):\n",
    "    res = np.empty(actual.shape)\n",
    "    for j in range(actual.shape[0]):\n",
    "        if actual[j] != 0:\n",
    "            res[j] = (actual[j] - predicted[j]) / actual[j]\n",
    "        else:\n",
    "            res[j] = predicted[j] / np.mean(actual)\n",
    "    return res\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs(percentage_error(np.asarray(y_true), np.asarray(y_pred)))) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8326871905249986\n"
     ]
    }
   ],
   "source": [
    "mape = mean_absolute_percentage_error(Y.tolist(), array.tolist())\n",
    "print(mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "myText = open(currentFile/metricsNum,'w')\n",
    "r2Text = \"r2: \" + str(r2)\n",
    "myText.write(r2Text)\n",
    "mapeText = \"\\nmape: \" + str(mape)\n",
    "myText.write(mapeText)\n",
    "myText.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
